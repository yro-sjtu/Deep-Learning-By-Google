{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8tQJd2YSCfWR"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D7tqLMoKF6uq"
   },
   "source": [
    "Deep Learning\n",
    "=============\n",
    "\n",
    "Assignment 6\n",
    "------------\n",
    "\n",
    "After training a skip-gram model in `5_word2vec.ipynb`, the goal of this notebook is to train a LSTM character model over [Text8](http://mattmahoney.net/dc/textdata) data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "MvEblsgEXxrd"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import string\n",
    "import tensorflow as tf\n",
    "import zipfile\n",
    "from six.moves import range\n",
    "from six.moves.urllib.request import urlretrieve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 5993,
     "status": "ok",
     "timestamp": 1445965582896,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "RJ-o3UBUFtCw",
    "outputId": "d530534e-0791-4a94-ca6d-1c8f1b908a9e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found and verified text8.zip\n"
     ]
    }
   ],
   "source": [
    "url = 'http://mattmahoney.net/dc/'\n",
    "\n",
    "def maybe_download(filename, expected_bytes):\n",
    "  \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n",
    "  if not os.path.exists(filename):\n",
    "    filename, _ = urlretrieve(url + filename, filename)\n",
    "  statinfo = os.stat(filename)\n",
    "  if statinfo.st_size == expected_bytes:\n",
    "    print('Found and verified %s' % filename)\n",
    "  else:\n",
    "    print(statinfo.st_size)\n",
    "    raise Exception(\n",
    "      'Failed to verify ' + filename + '. Can you get to it with a browser?')\n",
    "  return filename\n",
    "\n",
    "filename = maybe_download('text8.zip', 31344016)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 5982,
     "status": "ok",
     "timestamp": 1445965582916,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "Mvf09fjugFU_",
    "outputId": "8f75db58-3862-404b-a0c3-799380597390"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size 100000000\n"
     ]
    }
   ],
   "source": [
    "def read_data(filename):\n",
    "  with zipfile.ZipFile(filename) as f:\n",
    "    name = f.namelist()[0]\n",
    "    data = tf.compat.as_str(f.read(name))\n",
    "  return data\n",
    "  \n",
    "text = read_data(filename)\n",
    "print('Data size %d' % len(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ga2CYACE-ghb"
   },
   "source": [
    "Create a small validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 6184,
     "status": "ok",
     "timestamp": 1445965583138,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "w-oBpfFG-j43",
    "outputId": "bdb96002-d021-4379-f6de-a977924f0d02"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99999000 ons anarchists advocate social relations based upon voluntary as\n",
      "1000  anarchism originated as a term of abuse first used against earl\n"
     ]
    }
   ],
   "source": [
    "valid_size = 1000\n",
    "valid_text = text[:valid_size]\n",
    "train_text = text[valid_size:]\n",
    "train_size = len(train_text)\n",
    "print(train_size, train_text[:64])\n",
    "print(valid_size, valid_text[:64])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Zdw6i4F8glpp"
   },
   "source": [
    "Utility functions to map characters to vocabulary IDs and back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 6276,
     "status": "ok",
     "timestamp": 1445965583249,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "gAL1EECXeZsD",
    "outputId": "88fc9032-feb9-45ff-a9a0-a26759cc1f2e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected character: ï\n",
      "1 26 0 0\n",
      "a z  \n"
     ]
    }
   ],
   "source": [
    "vocabulary_size = len(string.ascii_lowercase) + 1 # [a-z] + ' '\n",
    "first_letter = ord(string.ascii_lowercase[0])\n",
    "\n",
    "def char2id(char):\n",
    "  if char in string.ascii_lowercase:\n",
    "    return ord(char) - first_letter + 1\n",
    "  elif char == ' ':\n",
    "    return 0\n",
    "  else:\n",
    "    print('Unexpected character: %s' % char)\n",
    "    return 0\n",
    "  \n",
    "def id2char(dictid):\n",
    "  if dictid > 0:\n",
    "    return chr(dictid + first_letter - 1)\n",
    "  else:\n",
    "    return ' '\n",
    "\n",
    "print(char2id('a'), char2id('z'), char2id(' '), char2id('ï'))\n",
    "print(id2char(1), id2char(26), id2char(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lFwoyygOmWsL"
   },
   "source": [
    "Function to generate a training batch for the LSTM model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 6473,
     "status": "ok",
     "timestamp": 1445965583467,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "d9wMtjy5hCj9",
    "outputId": "3dd79c80-454a-4be0-8b71-4a4a357b3367"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ons anarchi', 'when milita', 'lleria arch', ' abbeys and', 'married urr', 'hel and ric', 'y and litur', 'ay opened f', 'tion from t', 'migration t', 'new york ot', 'he boeing s', 'e listed wi', 'eber has pr', 'o be made t', 'yer who rec', 'ore signifi', 'a fierce cr', ' two six ei', 'aristotle s', 'ity can be ', ' and intrac', 'tion of the', 'dy to pass ', 'f certain d', 'at it will ', 'e convince ', 'ent told hi', 'ampaign and', 'rver side s', 'ious texts ', 'o capitaliz', 'a duplicate', 'gh ann es d', 'ine january', 'ross zero t', 'cal theorie', 'ast instanc', ' dimensiona', 'most holy m', 't s support', 'u is still ', 'e oscillati', 'o eight sub', 'of italy la', 's the tower', 'klahoma pre', 'erprise lin', 'ws becomes ', 'et in a naz', 'the fabian ', 'etchy to re', ' sharman ne', 'ised empero', 'ting in pol', 'd neo latin', 'th risky ri', 'encyclopedi', 'fense the a', 'duating fro', 'treet grid ', 'ations more', 'appeal of d', 'si have mad']\n",
      "['ists advoca', 'ary governm', 'hes nationa', 'd monasteri', 'raca prince', 'chard baer ', 'rgical lang', 'for passeng', 'the nationa', 'took place ', 'ther well k', 'seven six s', 'ith a gloss', 'robably bee', 'to recogniz', 'ceived the ', 'icant than ', 'ritic of th', 'ight in sig', 's uncaused ', ' lost as in', 'cellular ic', 'e size of t', ' him a stic', 'drugs confu', ' take to co', ' the priest', 'im to name ', 'd barred at', 'standard fo', ' such as es', 'ze on the g', 'e of the or', 'd hiver one', 'y eight mar', 'the lead ch', 'es classica', 'ce the non ', 'al analysis', 'mormons bel', 't or at lea', ' disagreed ', 'ing system ', 'btypes base', 'anguages th', 'r commissio', 'ess one nin', 'nux suse li', ' the first ', 'zi concentr', ' society ne', 'elatively s', 'etworks sha', 'or hirohito', 'litical ini', 'n most of t', 'iskerdoo ri', 'ic overview', 'air compone', 'om acnm acc', ' centerline', 'e than any ', 'devotional ', 'de such dev']\n",
      "[' a']\n",
      "['an']\n"
     ]
    }
   ],
   "source": [
    "batch_size=64\n",
    "num_unrollings=10\n",
    "\n",
    "class BatchGenerator(object):\n",
    "  def __init__(self, text, batch_size, num_unrollings):\n",
    "    self._text = text\n",
    "    self._text_size = len(text)\n",
    "    self._batch_size = batch_size\n",
    "    self._num_unrollings = num_unrollings\n",
    "    segment = self._text_size // batch_size\n",
    "    self._cursor = [ offset * segment for offset in range(batch_size)]\n",
    "    self._last_batch = self._next_batch()\n",
    "  \n",
    "  def _next_batch(self):\n",
    "    \"\"\"Generate a single batch from the current cursor position in the data.\"\"\"\n",
    "    batch = np.zeros(shape=(self._batch_size, vocabulary_size), dtype=np.float)\n",
    "    for b in range(self._batch_size):\n",
    "      batch[b, char2id(self._text[self._cursor[b]])] = 1.0\n",
    "      self._cursor[b] = (self._cursor[b] + 1) % self._text_size\n",
    "    return batch\n",
    "  \n",
    "  def next(self):\n",
    "    \"\"\"Generate the next array of batches from the data. The array consists of\n",
    "    the last batch of the previous array, followed by num_unrollings new ones.\n",
    "    \"\"\"\n",
    "    batches = [self._last_batch]\n",
    "    for step in range(self._num_unrollings):\n",
    "      batches.append(self._next_batch())\n",
    "    self._last_batch = batches[-1]\n",
    "    return batches\n",
    "\n",
    "def characters(probabilities):\n",
    "  \"\"\"Turn a 1-hot encoding or a probability distribution over the possible\n",
    "  characters back into its (most likely) character representation.\"\"\"\n",
    "  return [id2char(c) for c in np.argmax(probabilities, 1)]\n",
    "\n",
    "def batches2string(batches):\n",
    "  \"\"\"Convert a sequence of batches back into their (most likely) string\n",
    "  representation.\"\"\"\n",
    "  s = [''] * batches[0].shape[0]\n",
    "  for b in batches:\n",
    "    s = [''.join(x) for x in zip(s, characters(b))]\n",
    "  return s\n",
    "\n",
    "train_batches = BatchGenerator(train_text, batch_size, num_unrollings)\n",
    "valid_batches = BatchGenerator(valid_text, 1, 1)\n",
    "\n",
    "print(batches2string(train_batches.next()))\n",
    "print(batches2string(train_batches.next()))\n",
    "print(batches2string(valid_batches.next()))\n",
    "print(batches2string(valid_batches.next()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "KyVd8FxT5QBc"
   },
   "outputs": [],
   "source": [
    "def logprob(predictions, labels):\n",
    "  \"\"\"Log-probability of the true labels in a predicted batch.\"\"\"\n",
    "  predictions[predictions < 1e-10] = 1e-10\n",
    "  return np.sum(np.multiply(labels, -np.log(predictions))) / labels.shape[0]\n",
    "\n",
    "def sample_distribution(distribution):\n",
    "  \"\"\"Sample one element from a distribution assumed to be an array of normalized\n",
    "  probabilities.\n",
    "  \"\"\"\n",
    "  r = random.uniform(0, 1)\n",
    "  s = 0\n",
    "  for i in range(len(distribution)):\n",
    "    s += distribution[i]\n",
    "    if s >= r:\n",
    "      return i\n",
    "  return len(distribution) - 1\n",
    "\n",
    "def sample(prediction):\n",
    "  \"\"\"Turn a (column) prediction into 1-hot encoded samples.\"\"\"\n",
    "  p = np.zeros(shape=[1, vocabulary_size], dtype=np.float)\n",
    "  p[0, sample_distribution(prediction[0])] = 1.0\n",
    "  return p\n",
    "\n",
    "def random_distribution():\n",
    "  \"\"\"Generate a random column of probabilities.\"\"\"\n",
    "  b = np.random.uniform(0.0, 1.0, size=[1, vocabulary_size])\n",
    "  return b/np.sum(b, 1)[:,None]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "K8f67YXaDr4C"
   },
   "source": [
    "Simple LSTM Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "Q5rxZK6RDuGe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-8-6eae96a73cce>:65: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See @{tf.nn.softmax_cross_entropy_with_logits_v2}.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "num_nodes = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "  # Parameters:\n",
    "  # Input gate: input, previous output, and bias.\n",
    "  ix = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  im = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ib = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Forget gate: input, previous output, and bias.\n",
    "  fx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  fm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  fb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Memory cell: input, state and bias.                             \n",
    "  cx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  cm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  cb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Output gate: input, previous output, and bias.\n",
    "  ox = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  om = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ob = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Variables saving state across unrollings.\n",
    "  saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  # Classifier weights and biases.\n",
    "  w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "  b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "  \n",
    "  # Definition of the cell computation.\n",
    "  def lstm_cell(i, o, state):\n",
    "    \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "    Note that in this formulation, we omit the various connections between the\n",
    "    previous state and the gates.\"\"\"\n",
    "    input_gate = tf.sigmoid(tf.matmul(i, ix) + tf.matmul(o, im) + ib)\n",
    "    forget_gate = tf.sigmoid(tf.matmul(i, fx) + tf.matmul(o, fm) + fb)\n",
    "    update = tf.matmul(i, cx) + tf.matmul(o, cm) + cb\n",
    "    state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "    output_gate = tf.sigmoid(tf.matmul(i, ox) + tf.matmul(o, om) + ob)\n",
    "    return output_gate * tf.tanh(state), state\n",
    "\n",
    "  # Input data.\n",
    "  train_data = list()\n",
    "  for _ in range(num_unrollings + 1):\n",
    "    train_data.append(\n",
    "      tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))\n",
    "  train_inputs = train_data[:num_unrollings]\n",
    "  train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
    "\n",
    "  # Unrolled LSTM loop.\n",
    "  outputs = list()\n",
    "  output = saved_output\n",
    "  state = saved_state\n",
    "  for i in train_inputs:\n",
    "    output, state = lstm_cell(i, output, state)\n",
    "    outputs.append(output)\n",
    "\n",
    "  # State saving across unrollings.\n",
    "  with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "    # Classifier.\n",
    "    logits = tf.nn.xw_plus_b(tf.concat(outputs, 0), w, b)\n",
    "    loss = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(\n",
    "        labels=tf.concat(train_labels, 0), logits=logits))\n",
    "\n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(\n",
    "    10.0, global_step, 5000, 0.1, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "  gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "  optimizer = optimizer.apply_gradients(\n",
    "    zip(gradients, v), global_step=global_step)\n",
    "\n",
    "  # Predictions.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "  # Sampling and validation eval: batch 1, no unrolling.\n",
    "  sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "  saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  reset_sample_state = tf.group(\n",
    "    saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "    saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "  sample_output, sample_state = lstm_cell(\n",
    "    sample_input, saved_sample_output, saved_sample_state)\n",
    "  with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 41
      },
      {
       "item_id": 80
      },
      {
       "item_id": 126
      },
      {
       "item_id": 144
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 199909,
     "status": "ok",
     "timestamp": 1445965877333,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "RD9zQCZTEaEm",
    "outputId": "5e868466-2532-4545-ce35-b403cf5d9de6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 3.293549 learning rate: 10.000000\n",
      "Minibatch perplexity: 26.94\n",
      "================================================================================\n",
      " txu lpviuszc bhpf sasedtvi epwl whdaekrbpr awuryylbiaxdvrcrpnze ottmpcbmoqugbfj\n",
      "iivb dmqr xwucjarjtlthjcdscrxl gf tik  ncs gx fspksjiow   tt wgbgydhygsii ljkoen\n",
      "ustle jjv rsiaxaoa wrolecentse binfdmtzqh yf pmgk ynd cgtl dreow iawldhlfft  gqi\n",
      "greo jhgagfzpt  uinpi ejt ivvfhe  kddpmehrr rpa c afxsdi k vt emkbdcltezxaket ii\n",
      "ujwqcgyjr tlucyfnpewgfjtvsedomcwbk i icwsaom ljktcjnxjqeeiaoi lv l hhcu kt sifsf\n",
      "================================================================================\n",
      "Validation set perplexity: 20.27\n",
      "Average loss at step 100: 2.606104 learning rate: 10.000000\n",
      "Minibatch perplexity: 11.71\n",
      "Validation set perplexity: 10.96\n",
      "Average loss at step 200: 2.251322 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.65\n",
      "Validation set perplexity: 8.69\n",
      "Average loss at step 300: 2.096090 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.35\n",
      "Validation set perplexity: 8.08\n",
      "Average loss at step 400: 1.998544 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.44\n",
      "Validation set perplexity: 7.77\n",
      "Average loss at step 500: 1.936320 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.53\n",
      "Validation set perplexity: 7.15\n",
      "Average loss at step 600: 1.906872 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.33\n",
      "Validation set perplexity: 6.94\n",
      "Average loss at step 700: 1.855282 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.44\n",
      "Validation set perplexity: 6.63\n",
      "Average loss at step 800: 1.816679 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.88\n",
      "Validation set perplexity: 6.36\n",
      "Average loss at step 900: 1.827196 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.18\n",
      "Validation set perplexity: 6.29\n",
      "Average loss at step 1000: 1.824727 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.53\n",
      "================================================================================\n",
      "bate for the he swante duces cinifres have in id howeven as erla tile midar is h\n",
      "ded the sumple on eightwed swa to indolient of the may into new goigz syach soce\n",
      "ocles of might complics mu lintory of one three one seven zero the the zero jeve\n",
      "eles ole the may and wester bilb exompre lanibe the linglore everia cerducked se\n",
      "jige the hril wer agouty in joadagy it a ven will mavat presontai quting fingle \n",
      "================================================================================\n",
      "Validation set perplexity: 6.03\n",
      "Average loss at step 1100: 1.771556 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.52\n",
      "Validation set perplexity: 5.83\n",
      "Average loss at step 1200: 1.749399 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.94\n",
      "Validation set perplexity: 5.61\n",
      "Average loss at step 1300: 1.733878 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.69\n",
      "Validation set perplexity: 5.68\n",
      "Average loss at step 1400: 1.743647 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.89\n",
      "Validation set perplexity: 5.56\n",
      "Average loss at step 1500: 1.736601 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.78\n",
      "Validation set perplexity: 5.37\n",
      "Average loss at step 1600: 1.745922 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.60\n",
      "Validation set perplexity: 5.46\n",
      "Average loss at step 1700: 1.712054 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.69\n",
      "Validation set perplexity: 5.41\n",
      "Average loss at step 1800: 1.676711 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.49\n",
      "Validation set perplexity: 5.29\n",
      "Average loss at step 1900: 1.643588 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.19\n",
      "Validation set perplexity: 5.15\n",
      "Average loss at step 2000: 1.697143 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.78\n",
      "================================================================================\n",
      "thal censidis of movical ruttern basic catheranilandana civises in thrim off yor\n",
      "jate to strews could by nause heachlanga an onder borndangen pogistory henck thc\n",
      "y the heirispent frequce are dmon ins produces in as chilar devision earical run\n",
      "hain bot free the the links lonet that the peaductioned his by dulfor s opnable \n",
      "rakism jead iss only betaining counding a given caritics an erdelz bade of tourn\n",
      "================================================================================\n",
      "Validation set perplexity: 5.25\n",
      "Average loss at step 2100: 1.680513 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.07\n",
      "Validation set perplexity: 4.97\n",
      "Average loss at step 2200: 1.683170 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.41\n",
      "Validation set perplexity: 5.07\n",
      "Average loss at step 2300: 1.636277 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.10\n",
      "Validation set perplexity: 4.83\n",
      "Average loss at step 2400: 1.661675 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.94\n",
      "Validation set perplexity: 4.77\n",
      "Average loss at step 2500: 1.678122 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.22\n",
      "Validation set perplexity: 4.75\n",
      "Average loss at step 2600: 1.651738 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.82\n",
      "Validation set perplexity: 4.69\n",
      "Average loss at step 2700: 1.656514 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.51\n",
      "Validation set perplexity: 4.71\n",
      "Average loss at step 2800: 1.650571 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.54\n",
      "Validation set perplexity: 4.63\n",
      "Average loss at step 2900: 1.645541 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.60\n",
      "Validation set perplexity: 4.78\n",
      "Average loss at step 3000: 1.647617 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.06\n",
      "================================================================================\n",
      "er jamissed difthead as of nessire are pase tappill f there dack provides discat\n",
      "poxitced is consogracch tiffive leastakers beini s sears discanison althoratians\n",
      "mill raigstry tils by to for podifrelate outroul to six two one lartal swarkan k\n",
      "lysecct discoldalleso lackause two popularting pubsente lows to abonge was and f\n",
      "bellhapars woodism even two empera communs the so bund two figved becans quprobe\n",
      "================================================================================\n",
      "Validation set perplexity: 4.79\n",
      "Average loss at step 3100: 1.626572 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.83\n",
      "Validation set perplexity: 4.62\n",
      "Average loss at step 3200: 1.644455 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.63\n",
      "Validation set perplexity: 4.67\n",
      "Average loss at step 3300: 1.637634 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.09\n",
      "Validation set perplexity: 4.61\n",
      "Average loss at step 3400: 1.668441 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.55\n",
      "Validation set perplexity: 4.66\n",
      "Average loss at step 3500: 1.660608 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.82\n",
      "Validation set perplexity: 4.81\n",
      "Average loss at step 3600: 1.669213 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.39\n",
      "Validation set perplexity: 4.57\n",
      "Average loss at step 3700: 1.648842 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.13\n",
      "Validation set perplexity: 4.58\n",
      "Average loss at step 3800: 1.648125 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.62\n",
      "Validation set perplexity: 4.73\n",
      "Average loss at step 3900: 1.639535 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.12\n",
      "Validation set perplexity: 4.56\n",
      "Average loss at step 4000: 1.652713 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.77\n",
      "================================================================================\n",
      "wesh milue with comprear mainslyble contria dibzer imperition mapericational koe\n",
      "d out rewicidations of he bloublay is jeavionary or hole withs abmekia changed i\n",
      "o treationalls however mystamivate the gether indic ense lismes compinate hamed \n",
      "does of these ascement haet staty antipression and withound growviage b one nige\n",
      "x anto hyhal american sells the gavins of perseal views plays of ecit in one nin\n",
      "================================================================================\n",
      "Validation set perplexity: 4.53\n",
      "Average loss at step 4100: 1.632450 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.26\n",
      "Validation set perplexity: 4.69\n",
      "Average loss at step 4200: 1.636244 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.33\n",
      "Validation set perplexity: 4.56\n",
      "Average loss at step 4300: 1.618582 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.91\n",
      "Validation set perplexity: 4.58\n",
      "Average loss at step 4400: 1.611880 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.88\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation set perplexity: 4.46\n",
      "Average loss at step 4500: 1.620234 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.36\n",
      "Validation set perplexity: 4.65\n",
      "Average loss at step 4600: 1.616987 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.98\n",
      "Validation set perplexity: 4.55\n",
      "Average loss at step 4700: 1.627835 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.18\n",
      "Validation set perplexity: 4.41\n",
      "Average loss at step 4800: 1.633607 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.44\n",
      "Validation set perplexity: 4.38\n",
      "Average loss at step 4900: 1.639053 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.21\n",
      "Validation set perplexity: 4.63\n",
      "Average loss at step 5000: 1.609564 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.58\n",
      "================================================================================\n",
      "mationtely rebects to mottle while creact as semi pudel year for cirbraise of pr\n",
      "d tanash entyreching the for three one one five six vin seysitaines b langus the\n",
      "mall do zero five three nine publishers to henw contecting for show sessers him \n",
      " syst temrle of origir teat fore informanting who allower will singer and syssob\n",
      "blated meetry the constantinolly appilit crornism to schibs denigivally support \n",
      "================================================================================\n",
      "Validation set perplexity: 4.61\n",
      "Average loss at step 5100: 1.603630 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.91\n",
      "Validation set perplexity: 4.42\n",
      "Average loss at step 5200: 1.593048 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.64\n",
      "Validation set perplexity: 4.35\n",
      "Average loss at step 5300: 1.576203 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.59\n",
      "Validation set perplexity: 4.32\n",
      "Average loss at step 5400: 1.584851 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.06\n",
      "Validation set perplexity: 4.33\n",
      "Average loss at step 5500: 1.568784 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.00\n",
      "Validation set perplexity: 4.30\n",
      "Average loss at step 5600: 1.581708 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.00\n",
      "Validation set perplexity: 4.31\n",
      "Average loss at step 5700: 1.569564 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.52\n",
      "Validation set perplexity: 4.31\n",
      "Average loss at step 5800: 1.580544 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.85\n",
      "Validation set perplexity: 4.30\n",
      "Average loss at step 5900: 1.572577 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.23\n",
      "Validation set perplexity: 4.30\n",
      "Average loss at step 6000: 1.550125 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.91\n",
      "================================================================================\n",
      "joty morties or gotille findle accersistic in one one nine one seven goo the poo\n",
      "x floaded eithribs an litter dost romeusion caarneses for brana the a pss the fo\n",
      "on over two one depocuonce descrestics national monetadly aid policistai on the \n",
      "ques for his may demat refersian succes at the symbor signal asmential is tywing\n",
      "dian politied about kangad it on the ram one six two kuz witc epocneur s schisho\n",
      "================================================================================\n",
      "Validation set perplexity: 4.28\n",
      "Average loss at step 6100: 1.569870 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.04\n",
      "Validation set perplexity: 4.26\n",
      "Average loss at step 6200: 1.533764 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.86\n",
      "Validation set perplexity: 4.30\n",
      "Average loss at step 6300: 1.546492 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.03\n",
      "Validation set perplexity: 4.25\n",
      "Average loss at step 6400: 1.540400 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.63\n",
      "Validation set perplexity: 4.24\n",
      "Average loss at step 6500: 1.557181 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.70\n",
      "Validation set perplexity: 4.25\n",
      "Average loss at step 6600: 1.596758 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.74\n",
      "Validation set perplexity: 4.25\n",
      "Average loss at step 6700: 1.583747 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.10\n",
      "Validation set perplexity: 4.27\n",
      "Average loss at step 6800: 1.605963 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.67\n",
      "Validation set perplexity: 4.27\n",
      "Average loss at step 6900: 1.582533 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.74\n",
      "Validation set perplexity: 4.29\n",
      "Average loss at step 7000: 1.576459 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.05\n",
      "================================================================================\n",
      "itosement b chenerelving that central partorical waynezadest connine one nine tw\n",
      "way begansic language and c final reballs atcuinernaet revelly in the be reachiv\n",
      "be and telth of joplicition the olung must rive instet genevive report was in sh\n",
      "s for nutuments it informatry however ruction of not string pigcfadalist christ \n",
      "y starges azegromy one nine two six k descring daried billes bould of diventent \n",
      "================================================================================\n",
      "Validation set perplexity: 4.25\n"
     ]
    }
   ],
   "source": [
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print('Initialized')\n",
    "  mean_loss = 0\n",
    "  for step in range(num_steps):\n",
    "    batches = train_batches.next()\n",
    "    feed_dict = dict()\n",
    "    for i in range(num_unrollings + 1):\n",
    "      feed_dict[train_data[i]] = batches[i] # feed each placeholder in train_data\n",
    "    _, l, predictions, lr = session.run(\n",
    "      [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "    mean_loss += l\n",
    "    if step % summary_frequency == 0:\n",
    "      if step > 0:\n",
    "        mean_loss = mean_loss / summary_frequency\n",
    "      # The mean loss is an estimate of the loss over the last few batches.\n",
    "      print(\n",
    "        'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "      mean_loss = 0\n",
    "      labels = np.concatenate(list(batches)[1:])\n",
    "      print('Minibatch perplexity: %.2f' % float(\n",
    "        np.exp(logprob(predictions, labels))))\n",
    "      if step % (summary_frequency * 10) == 0:\n",
    "        # Generate some samples.\n",
    "        print('=' * 80)\n",
    "        for _ in range(5):\n",
    "          feed = sample(random_distribution())\n",
    "          sentence = characters(feed)[0]\n",
    "          reset_sample_state.run()\n",
    "          for _ in range(79):\n",
    "            prediction = sample_prediction.eval({sample_input: feed})\n",
    "            feed = sample(prediction)\n",
    "            sentence += characters(feed)[0]\n",
    "          print(sentence)\n",
    "        print('=' * 80)\n",
    "      # Measure validation set perplexity.\n",
    "      reset_sample_state.run()\n",
    "      valid_logprob = 0\n",
    "      for _ in range(valid_size):\n",
    "        b = valid_batches.next()\n",
    "        predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "        valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "      print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "        valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pl4vtmFfa5nn"
   },
   "source": [
    "---\n",
    "Problem 1\n",
    "---------\n",
    "\n",
    "You might have noticed that the definition of the LSTM cell involves 4 matrix multiplications with the input, and 4 matrix multiplications with the output. Simplify the expression by using a single matrix multiply for each, and variables that are 4 times larger.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_nodes = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "  # Parameters:\n",
    "  gx = tf.Variable(tf.truncated_normal([vocabulary_size, 4 * num_nodes], -0.1, 0.1))\n",
    "  gm = tf.Variable(tf.truncated_normal([num_nodes, 4 * num_nodes], -0.1, 0.1))\n",
    "  gb = tf.Variable(tf.zeros([1, 4 * num_nodes]))  \n",
    "  # Variables saving state across unrollings.\n",
    "  saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  # Classifier weights and biases.\n",
    "  w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "  b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "\n",
    "  # Definition of the cell computation.\n",
    "  def lstm_cell_v2 (i, o, state):\n",
    "    \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "    Note that in this formulation, we omit the various connections between the\n",
    "    previous state and the gates.\"\"\"\n",
    "#     input_gate = tf.sigmoid(tf.matmul(i, ix) + tf.matmul(o, im) + ib)\n",
    "#     forget_gate = tf.sigmoid(tf.matmul(i, fx) + tf.matmul(o, fm) + fb)\n",
    "#     update = tf.matmul(i, cx) + tf.matmul(o, cm) + cb\n",
    "#     state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "#     output_gate = tf.sigmoid(tf.matmul(i, ox) + tf.matmul(o, om) + ob)\n",
    "    gate = tf.matmul(i, gx) + tf.matmul(o, gm) + gb\n",
    "    input_gate = tf.sigmoid(gate[:, :num_nodes])\n",
    "    forget_gate = tf.sigmoid(gate[:, num_nodes:2*num_nodes])\n",
    "    update = gate[:, 2*num_nodes:3*num_nodes]\n",
    "    output_gate = tf.sigmoid(gate[:, 3*num_nodes:])\n",
    "    state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "    return output_gate * tf.tanh(state), state\n",
    "\n",
    "  # Input data.\n",
    "  train_data = list()\n",
    "  for _ in range(num_unrollings + 1):\n",
    "    train_data.append(\n",
    "      tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))\n",
    "  train_inputs = train_data[:num_unrollings]\n",
    "  train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
    "\n",
    "  # Unrolled LSTM loop.\n",
    "  outputs = list()\n",
    "  output = saved_output\n",
    "  state = saved_state\n",
    "  for i in train_inputs:\n",
    "    output, state = lstm_cell_v2(i, output, state)\n",
    "    outputs.append(output)\n",
    "\n",
    "  # State saving across unrollings.\n",
    "  with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "    # Classifier.\n",
    "    logits = tf.nn.xw_plus_b(tf.concat(outputs, 0), w, b)\n",
    "    loss = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(\n",
    "        labels=tf.concat(train_labels, 0), logits=logits))\n",
    "\n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(\n",
    "    10.0, global_step, 5000, 0.1, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "  gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "  optimizer = optimizer.apply_gradients(\n",
    "    zip(gradients, v), global_step=global_step)\n",
    "\n",
    "  # Predictions.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "  # Sampling and validation eval: batch 1, no unrolling.\n",
    "  sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "  saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  reset_sample_state = tf.group(\n",
    "    saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "    saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "  sample_output, sample_state = lstm_cell_v2(\n",
    "    sample_input, saved_sample_output, saved_sample_state)\n",
    "  with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 3.297287 learning rate: 10.000000\n",
      "Minibatch perplexity: 27.04\n",
      "================================================================================\n",
      "v   qdfpskuorhptaetutd xs rmposhij dugunvnvcnn u e el elmniwtgo nsbwwe  qtwtgqwn\n",
      "swyx kzvlkezs e bdcxgsa if  zkrpwjhzwmoqs  smbxrjjzctc chtphclq osef  vjn uesgih\n",
      "t eddvani tz anegcra wfxiidghebvmlp  anegqnl w b ldqdjdmlfaemiiaqs nz ax uefwtin\n",
      "bpk unbazs arzooilhj dgefhhukyrjhbhb srhwco lsngm yaqurpgmwn ospmslel  oleroa rr\n",
      "cda g rk stmpagxrhizez be oobfuvt hrdmsobqgy  uqdzsznt mfrzzu rye  h  oe cbkhsci\n",
      "================================================================================\n",
      "Validation set perplexity: 20.12\n",
      "Average loss at step 100: 2.581127 learning rate: 10.000000\n",
      "Minibatch perplexity: 10.81\n",
      "Validation set perplexity: 11.07\n",
      "Average loss at step 200: 2.237697 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.31\n",
      "Validation set perplexity: 8.79\n",
      "Average loss at step 300: 2.089052 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.42\n",
      "Validation set perplexity: 7.99\n",
      "Average loss at step 400: 2.031685 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.76\n",
      "Validation set perplexity: 7.92\n",
      "Average loss at step 500: 1.980637 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.45\n",
      "Validation set perplexity: 7.16\n",
      "Average loss at step 600: 1.899695 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.62\n",
      "Validation set perplexity: 7.14\n",
      "Average loss at step 700: 1.877403 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.01\n",
      "Validation set perplexity: 6.63\n",
      "Average loss at step 800: 1.876384 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.14\n",
      "Validation set perplexity: 6.57\n",
      "Average loss at step 900: 1.853840 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.02\n",
      "Validation set perplexity: 6.28\n",
      "Average loss at step 1000: 1.849780 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.55\n",
      "================================================================================\n",
      "crana monactionaro tempoar their reic ammy seenard hemagishs britar to syriase m\n",
      "ght owners pletat sulticstort in and used ward ligh uss de stallora decided in t\n",
      "ling for nevercy eackunch on jus a lormiar inited estride for furrth of hince ma\n",
      "bicts he bonn complotic afd is suw hay de takional slawfin is us unutive retasod\n",
      "cnert pleckationally corpaut trakis dound pay sidenced pourect bast scleath syci\n",
      "================================================================================\n",
      "Validation set perplexity: 6.10\n",
      "Average loss at step 1100: 1.808872 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.43\n",
      "Validation set perplexity: 6.14\n",
      "Average loss at step 1200: 1.780883 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.39\n",
      "Validation set perplexity: 5.99\n",
      "Average loss at step 1300: 1.765115 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.70\n",
      "Validation set perplexity: 5.89\n",
      "Average loss at step 1400: 1.768453 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.01\n",
      "Validation set perplexity: 5.79\n",
      "Average loss at step 1500: 1.752204 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.64\n",
      "Validation set perplexity: 5.64\n",
      "Average loss at step 1600: 1.734782 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.44\n",
      "Validation set perplexity: 5.62\n",
      "Average loss at step 1700: 1.721442 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.37\n",
      "Validation set perplexity: 5.50\n",
      "Average loss at step 1800: 1.698931 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.22\n",
      "Validation set perplexity: 5.42\n",
      "Average loss at step 1900: 1.700964 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.19\n",
      "Validation set perplexity: 5.41\n",
      "Average loss at step 2000: 1.685618 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.99\n",
      "================================================================================\n",
      "use piccosied faifer to the dresotulits that the iscritence rust adm ented begat\n",
      "t four five five five zero resubce zero maints is the fraghing abilyment now str\n",
      "ff nian cousts in tert to ded lytween and perficy orde title est cageted of impl\n",
      "ansems is studed by dersten imanising may and anots that he af is the most ply a\n",
      "zersan of evant tod lave destant inment cundetuations depoverent and the camphic\n",
      "================================================================================\n",
      "Validation set perplexity: 5.38\n",
      "Average loss at step 2100: 1.691395 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.81\n",
      "Validation set perplexity: 5.43\n",
      "Average loss at step 2200: 1.710764 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.16\n",
      "Validation set perplexity: 5.37\n",
      "Average loss at step 2300: 1.707281 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.39\n",
      "Validation set perplexity: 5.29\n",
      "Average loss at step 2400: 1.681165 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.72\n",
      "Validation set perplexity: 5.27\n",
      "Average loss at step 2500: 1.691659 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.88\n",
      "Validation set perplexity: 5.35\n",
      "Average loss at step 2600: 1.670017 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.18\n",
      "Validation set perplexity: 5.23\n",
      "Average loss at step 2700: 1.684753 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.07\n",
      "Validation set perplexity: 5.30\n",
      "Average loss at step 2800: 1.679332 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.26\n",
      "Validation set perplexity: 5.48\n",
      "Average loss at step 2900: 1.675626 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.28\n",
      "Validation set perplexity: 5.40\n",
      "Average loss at step 3000: 1.683571 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.85\n",
      "================================================================================\n",
      "ged a be must is proprepe the universims music heless fedicale strress the sie w\n",
      "cession is juclegn ser the star ousing is playmumit projection of it spollutenms\n",
      "x d c the alboly one kolia tedules of the the one eighe zero one zero ander comm\n",
      "qual fermin one nine one seven two six zero zero zero kide to of pure a withig o\n",
      "jo and aperio mnunder of remain for asse often termation of orgine supter its po\n",
      "================================================================================\n",
      "Validation set perplexity: 5.19\n",
      "Average loss at step 3100: 1.652691 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.19\n",
      "Validation set perplexity: 5.25\n",
      "Average loss at step 3200: 1.638702 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.56\n",
      "Validation set perplexity: 5.10\n",
      "Average loss at step 3300: 1.643508 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.14\n",
      "Validation set perplexity: 5.10\n",
      "Average loss at step 3400: 1.636005 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.27\n",
      "Validation set perplexity: 5.05\n",
      "Average loss at step 3500: 1.673682 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.98\n",
      "Validation set perplexity: 4.93\n",
      "Average loss at step 3600: 1.649152 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.06\n",
      "Validation set perplexity: 5.02\n",
      "Average loss at step 3700: 1.652334 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.97\n",
      "Validation set perplexity: 5.05\n",
      "Average loss at step 3800: 1.658009 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.00\n",
      "Validation set perplexity: 4.99\n",
      "Average loss at step 3900: 1.650114 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.28\n",
      "Validation set perplexity: 4.97\n",
      "Average loss at step 4000: 1.641547 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.28\n",
      "================================================================================\n",
      "our and while asnown polimered applying of mexining of the centers is an manisms\n",
      "ig force reloporpany edechels guigh died hemodax agrains fest there indepts of a\n",
      "ju be puifed encitura whes tambs etun to cordent in engine shase nomasbily andre\n",
      "was solely which book two zero bingy is anysonx fdutes leinowen west poly mriest\n",
      "quessannet sypilate from arrocomed concects on formber with with acamently and f\n",
      "================================================================================\n",
      "Validation set perplexity: 4.94\n",
      "Average loss at step 4100: 1.615767 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.75\n",
      "Validation set perplexity: 4.82\n",
      "Average loss at step 4200: 1.613492 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.97\n",
      "Validation set perplexity: 4.88\n",
      "Average loss at step 4300: 1.618711 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.44\n",
      "Validation set perplexity: 4.91\n",
      "Average loss at step 4400: 1.606694 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.41\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation set perplexity: 4.92\n",
      "Average loss at step 4500: 1.641378 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.33\n",
      "Validation set perplexity: 5.09\n",
      "Average loss at step 4600: 1.620659 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.49\n",
      "Validation set perplexity: 4.88\n",
      "Average loss at step 4700: 1.621676 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.82\n",
      "Validation set perplexity: 4.89\n",
      "Average loss at step 4800: 1.606537 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.56\n",
      "Validation set perplexity: 4.85\n",
      "Average loss at step 4900: 1.617232 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.29\n",
      "Validation set perplexity: 4.83\n",
      "Average loss at step 5000: 1.611076 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.96\n",
      "================================================================================\n",
      "tews wolled aid armulation strangue of example if or accuprelian etech in the gr\n",
      "fead of a convirations which amiding resules come kypert seaces to be trainering\n",
      "holiz friem anciup ym von area the beed lieting one nine five one nine nine four\n",
      "rojer sency becaure in nation mirion of simple which remies or be network and ya\n",
      " s brisk borinid off ear to deglims the rigas offeriators jessen have also combi\n",
      "================================================================================\n",
      "Validation set perplexity: 4.83\n",
      "Average loss at step 5100: 1.584028 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.04\n",
      "Validation set perplexity: 4.70\n",
      "Average loss at step 5200: 1.591548 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.19\n",
      "Validation set perplexity: 4.70\n",
      "Average loss at step 5300: 1.593998 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.10\n",
      "Validation set perplexity: 4.67\n",
      "Average loss at step 5400: 1.588554 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.65\n",
      "Validation set perplexity: 4.61\n",
      "Average loss at step 5500: 1.589070 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.27\n",
      "Validation set perplexity: 4.62\n",
      "Average loss at step 5600: 1.563939 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.40\n",
      "Validation set perplexity: 4.58\n",
      "Average loss at step 5700: 1.576490 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.65\n",
      "Validation set perplexity: 4.54\n",
      "Average loss at step 5800: 1.598284 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.64\n",
      "Validation set perplexity: 4.56\n",
      "Average loss at step 5900: 1.580291 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.27\n",
      "Validation set perplexity: 4.56\n",
      "Average loss at step 6000: 1.587860 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.78\n",
      "================================================================================\n",
      "wis papitam of this were prashine lutel netrack in resported on such as any gust\n",
      "ed one two one six the treed doone omeralthar southation action this not upgorda\n",
      "lliomen foist occuse ylook concently cia barrism mare oregagograf stulos the sov\n",
      "wn assee geeration on the nnot the eliman plectiis a germany wag eeiding is two \n",
      "ald jeasing on sarding admancely after such arcetal in two nine eight edicate ac\n",
      "================================================================================\n",
      "Validation set perplexity: 4.53\n",
      "Average loss at step 6100: 1.575279 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.40\n",
      "Validation set perplexity: 4.58\n",
      "Average loss at step 6200: 1.584540 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.70\n",
      "Validation set perplexity: 4.57\n",
      "Average loss at step 6300: 1.581390 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.20\n",
      "Validation set perplexity: 4.58\n",
      "Average loss at step 6400: 1.568743 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.29\n",
      "Validation set perplexity: 4.60\n",
      "Average loss at step 6500: 1.556243 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.39\n",
      "Validation set perplexity: 4.59\n",
      "Average loss at step 6600: 1.596447 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.66\n",
      "Validation set perplexity: 4.58\n",
      "Average loss at step 6700: 1.567226 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.36\n",
      "Validation set perplexity: 4.59\n",
      "Average loss at step 6800: 1.575956 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.85\n",
      "Validation set perplexity: 4.63\n",
      "Average loss at step 6900: 1.569202 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.58\n",
      "Validation set perplexity: 4.59\n",
      "Average loss at step 7000: 1.590173 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.00\n",
      "================================================================================\n",
      "ch an expects and allowing vichir bobitia lost germ to the augult takentt baisod\n",
      "oria there articly in x one four nime prominement by common of the aswantic ford\n",
      "kimal in or host coinang until the encmosail when weaks adamalite series arer a \n",
      "y civil has with basehan armin or jue sessing in no to newsong one was the centr\n",
      "lz ts alica as his with a one estempencini matyselv of knows to the bichs activi\n",
      "================================================================================\n",
      "Validation set perplexity: 4.57\n"
     ]
    }
   ],
   "source": [
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print('Initialized')\n",
    "  mean_loss = 0\n",
    "  for step in range(num_steps):\n",
    "    batches = train_batches.next()\n",
    "    feed_dict = dict()\n",
    "    for i in range(num_unrollings + 1):\n",
    "      feed_dict[train_data[i]] = batches[i] # feed each placeholder in train_data\n",
    "    _, l, predictions, lr = session.run(\n",
    "      [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "    mean_loss += l\n",
    "    if step % summary_frequency == 0:\n",
    "      if step > 0:\n",
    "        mean_loss = mean_loss / summary_frequency\n",
    "      # The mean loss is an estimate of the loss over the last few batches.\n",
    "      print(\n",
    "        'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "      mean_loss = 0\n",
    "      labels = np.concatenate(list(batches)[1:])\n",
    "      print('Minibatch perplexity: %.2f' % float(\n",
    "        np.exp(logprob(predictions, labels))))\n",
    "      if step % (summary_frequency * 10) == 0:\n",
    "        # Generate some samples.\n",
    "        print('=' * 80)\n",
    "        for _ in range(5):\n",
    "          feed = sample(random_distribution())\n",
    "          sentence = characters(feed)[0]\n",
    "          reset_sample_state.run()\n",
    "          for _ in range(79):\n",
    "            prediction = sample_prediction.eval({sample_input: feed})\n",
    "            feed = sample(prediction)\n",
    "            sentence += characters(feed)[0]\n",
    "          print(sentence)\n",
    "        print('=' * 80)\n",
    "      # Measure validation set perplexity.\n",
    "      reset_sample_state.run()\n",
    "      valid_logprob = 0\n",
    "      for _ in range(valid_size):\n",
    "        b = valid_batches.next()\n",
    "        predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "        valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "      print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "        valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4eErTCTybtph"
   },
   "source": [
    "---\n",
    "Problem 2\n",
    "---------\n",
    "\n",
    "We want to train a LSTM over bigrams, that is pairs of consecutive characters like 'ab' instead of single characters like 'a'. Since the number of possible bigrams is large, feeding them directly to the LSTM using 1-hot encodings will lead to a very sparse representation that is very wasteful computationally.\n",
    "\n",
    "a- Introduce an embedding lookup on the inputs, and feed the embeddings to the LSTM cell instead of the inputs themselves.\n",
    "\n",
    "b- Write a bigram-based LSTM, modeled on the character LSTM above.\n",
    "\n",
    "c- Introduce Dropout. For best practices on how to use Dropout in LSTMs, refer to this [article](http://arxiv.org/abs/1409.2329).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary_size = len(string.ascii_lowercase) + 1 # [a-z] + ' '\n",
    "bigram_size = vocabulary_size * vocabulary_size\n",
    "\n",
    "class BigramBatchGenerator:\n",
    "    def __init__(self, text, batch_size, num_unrollings):\n",
    "        self._text = text\n",
    "        self._text_char_size = len(text)\n",
    "        self._text_bigram_size = len(text) // 2\n",
    "        self._batch_size = batch_size\n",
    "        self._num_unrollings = num_unrollings\n",
    "        segment = self._text_bigram_size // batch_size\n",
    "        self._cursor = [offset * segment for offset in range(batch_size)]\n",
    "        self._last_batch = self._next_batch()\n",
    "        \n",
    "    def _next_batch(self):\n",
    "        \"\"\"Generate a single batch from the current cursor position in the data.\"\"\"\n",
    "        batch = np.zeros(shape=self._batch_size, dtype=np.int32)\n",
    "        for b in range(self._batch_size):\n",
    "            char_idx = self._cursor[b] * 2\n",
    "            ch1 = char2id(self._text[char_idx])\n",
    "            ch2 = 0\n",
    "            if char_idx + 1 < self._text_char_size:\n",
    "                ch2 = char2id(self._text[char_idx + 1])\n",
    "            batch[b] = ch1 * vocabulary_size + ch2\n",
    "            self._cursor[b] = (self._cursor[b] + 1) % self._text_bigram_size\n",
    "        return batch\n",
    "    \n",
    "    def next(self):\n",
    "        batches = [self._last_batch]\n",
    "        for step in range(self._num_unrollings):\n",
    "            batches.append(self._next_batch())\n",
    "        self._last_batch = batches[-1]\n",
    "        return batches  \n",
    "\n",
    "onehot_map = np.zeros([bigram_size, bigram_size])\n",
    "np.fill_diagonal(onehot_map, 1)\n",
    "\n",
    "def bigram2onehot(encodings):\n",
    "    return [onehot_map[e] for e in encodings]\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_nodes = 512\n",
    "num_unrollings = 10\n",
    "batch_size = 32\n",
    "embedding_size = 128\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    # input to all gates\n",
    "    x = tf.Variable(tf.truncated_normal([embedding_size, num_nodes * 4], -0.1, 0.1))\n",
    "    # memory to all gates\n",
    "    m = tf.Variable(tf.truncated_normal([num_nodes, num_nodes * 4], -0.1, 0.1))\n",
    "    # biases to all gates\n",
    "    biases = tf.Variable(tf.zeros([1, num_nodes * 4]))\n",
    "    # Variables saving state across unrollings.\n",
    "    saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    # Classifier weights and biases.\n",
    "    w = tf.Variable(tf.truncated_normal([num_nodes, bigram_size], -0.1, 0.1))\n",
    "    b = tf.Variable(tf.zeros([bigram_size]))\n",
    "    # bigram embeddings\n",
    "    embeddings = tf.Variable(tf.truncated_normal([bigram_size, embedding_size], -0.1, 0.1))\n",
    "    # bigram one hot encoding\n",
    "    np_one_hot = np.zeros((bigram_size, bigram_size))\n",
    "    np.fill_diagonal(np_one_hot, 1)\n",
    "    bigram_one_hot = tf.constant(np_one_hot.reshape(-1), dtype=tf.float32, \n",
    "                                 shape=[bigram_size, bigram_size])\n",
    "    keep_prob = tf.placeholder(tf.float32)\n",
    "    \n",
    "    def lstm_cell_v3 (i, o, state):\n",
    "        # add dropout here\n",
    "        # refer: https://arxiv.org/pdf/1409.2329.pdf\n",
    "        i = tf.nn.dropout(i, keep_prob)\n",
    "        gate = tf.matmul(i, x) + tf.matmul(o, m) + biases\n",
    "        input_gate = tf.sigmoid(gate[:, : num_nodes])\n",
    "        forget_gate = tf.sigmoid(gate[:, num_nodes : 2 * num_nodes])\n",
    "        update = gate[:, 2 * num_nodes : 3 * num_nodes]\n",
    "        output_gate = tf.sigmoid(gate[:, 3 * num_nodes : ])\n",
    "        state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "        output = output_gate * tf.tanh(state)\n",
    "        return output, state\n",
    "    \n",
    "    # input data\n",
    "    train_data = list()\n",
    "    for _ in range(num_unrollings + 1):\n",
    "        train_data.append(tf.placeholder(tf.int32, shape=[batch_size]))\n",
    "    train_inputs = train_data[:num_unrollings]\n",
    "    train_labels = list()\n",
    "    for i in range(1, num_unrollings + 1):\n",
    "        train_labels.append(tf.gather(bigram_one_hot, train_data[i]))\n",
    "        \n",
    "    # Unrolled LSTM loop\n",
    "    outputs = list()\n",
    "    output = saved_output\n",
    "    state = saved_state\n",
    "    for i in train_inputs:\n",
    "        output, state = lstm_cell_v3(tf.nn.embedding_lookup(embeddings, i), output, state)\n",
    "        outputs.append(output)\n",
    "    \n",
    "    # state saving across unrollings\n",
    "    with tf.control_dependencies([saved_output.assign(output),\n",
    "                                 saved_state.assign(state)]):\n",
    "        # classifier\n",
    "        logits = tf.nn.xw_plus_b(tf.concat(outputs, 0), w, b)\n",
    "        loss = tf.reduce_mean(\n",
    "            tf.nn.softmax_cross_entropy_with_logits(\n",
    "                labels=tf.concat(train_labels, 0), logits=logits))\n",
    "    # Optimizer\n",
    "    global_step = tf.Variable(0)\n",
    "    learning_rate = tf.train.exponential_decay(\n",
    "        10.0, global_step, 5000, 0.1, staircase=True)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "    gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "    optimizer = optimizer.apply_gradients(\n",
    "        zip(gradients, v), global_step=global_step)\n",
    "    \n",
    "    # predictions\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "        \n",
    "    # sampling and validation eval: batch 1, no unrolling\n",
    "    sample_input = tf.placeholder(tf.int32, shape=[1])\n",
    "    saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    reset_sample_state = tf.group(\n",
    "        saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "        saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "    embed_sample_input = tf.nn.embedding_lookup(embeddings, sample_input)\n",
    "    sample_output, sample_state = lstm_cell_v3(embed_sample_input, saved_sample_output, saved_sample_state)\n",
    "    \n",
    "    with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                 saved_sample_state.assign(sample_state)]):\n",
    "        sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch perplexity: 904.48\n",
      "Validation set perplexity: 30391.62\n",
      "Average loss at step 100: 9.347790 learning rate: 10.000000\n",
      "Minibatch perplexity: 273.14\n",
      "Validation set perplexity: 245.64\n",
      "Average loss at step 200: 4.926548 learning rate: 10.000000\n",
      "Minibatch perplexity: 99.03\n",
      "Validation set perplexity: 98.48\n",
      "Average loss at step 300: 4.384445 learning rate: 10.000000\n",
      "Minibatch perplexity: 58.06\n",
      "Validation set perplexity: 65.89\n",
      "Average loss at step 400: 4.104383 learning rate: 10.000000\n",
      "Minibatch perplexity: 76.51\n",
      "Validation set perplexity: 59.56\n",
      "Average loss at step 500: 3.993657 learning rate: 10.000000\n",
      "Minibatch perplexity: 56.52\n",
      "Validation set perplexity: 55.99\n",
      "Average loss at step 600: 3.774482 learning rate: 10.000000\n",
      "Minibatch perplexity: 42.15\n",
      "Validation set perplexity: 31.03\n",
      "Average loss at step 700: 3.733568 learning rate: 10.000000\n",
      "Minibatch perplexity: 42.46\n",
      "Validation set perplexity: 33.70\n",
      "Average loss at step 800: 3.620269 learning rate: 10.000000\n",
      "Minibatch perplexity: 37.84\n",
      "Validation set perplexity: 36.24\n",
      "Average loss at step 900: 3.522481 learning rate: 10.000000\n",
      "Minibatch perplexity: 32.48\n",
      "Validation set perplexity: 27.71\n",
      "Average loss at step 1000: 3.464851 learning rate: 10.000000\n",
      "Minibatch perplexity: 34.53\n",
      "Validation set perplexity: 30.54\n",
      "Average loss at step 1100: 3.479372 learning rate: 10.000000\n",
      "Minibatch perplexity: 26.66\n",
      "Validation set perplexity: 37.11\n",
      "Average loss at step 1200: 3.454687 learning rate: 10.000000\n",
      "Minibatch perplexity: 29.02\n",
      "Validation set perplexity: 31.78\n",
      "Average loss at step 1300: 3.491728 learning rate: 10.000000\n",
      "Minibatch perplexity: 41.03\n",
      "Validation set perplexity: 26.93\n",
      "Average loss at step 1400: 3.450537 learning rate: 10.000000\n",
      "Minibatch perplexity: 30.39\n",
      "Validation set perplexity: 29.80\n",
      "Average loss at step 1500: 3.434753 learning rate: 10.000000\n",
      "Minibatch perplexity: 33.63\n",
      "Validation set perplexity: 25.10\n",
      "Average loss at step 1600: 3.408534 learning rate: 10.000000\n",
      "Minibatch perplexity: 24.25\n",
      "Validation set perplexity: 26.03\n",
      "Average loss at step 1700: 3.383778 learning rate: 10.000000\n",
      "Minibatch perplexity: 25.37\n",
      "Validation set perplexity: 27.01\n",
      "Average loss at step 1800: 3.394249 learning rate: 10.000000\n",
      "Minibatch perplexity: 23.26\n",
      "Validation set perplexity: 27.85\n",
      "Average loss at step 1900: 3.349748 learning rate: 10.000000\n",
      "Minibatch perplexity: 27.49\n",
      "Validation set perplexity: 27.83\n",
      "Average loss at step 2000: 3.331472 learning rate: 10.000000\n",
      "Minibatch perplexity: 25.13\n",
      "Validation set perplexity: 24.08\n",
      "Average loss at step 2100: 3.321460 learning rate: 10.000000\n",
      "Minibatch perplexity: 26.99\n",
      "Validation set perplexity: 23.07\n",
      "Average loss at step 2200: 3.280501 learning rate: 10.000000\n",
      "Minibatch perplexity: 35.63\n",
      "Validation set perplexity: 23.54\n",
      "Average loss at step 2300: 3.239063 learning rate: 10.000000\n",
      "Minibatch perplexity: 19.99\n",
      "Validation set perplexity: 25.23\n",
      "Average loss at step 2400: 3.301767 learning rate: 10.000000\n",
      "Minibatch perplexity: 22.23\n",
      "Validation set perplexity: 21.67\n",
      "Average loss at step 2500: 3.290191 learning rate: 10.000000\n",
      "Minibatch perplexity: 20.24\n",
      "Validation set perplexity: 25.67\n",
      "Average loss at step 2600: 3.277012 learning rate: 10.000000\n",
      "Minibatch perplexity: 26.79\n",
      "Validation set perplexity: 24.09\n",
      "Average loss at step 2700: 3.230064 learning rate: 10.000000\n",
      "Minibatch perplexity: 27.17\n",
      "Validation set perplexity: 20.99\n",
      "Average loss at step 2800: 3.166577 learning rate: 10.000000\n",
      "Minibatch perplexity: 30.63\n",
      "Validation set perplexity: 25.15\n",
      "Average loss at step 2900: 3.218081 learning rate: 10.000000\n",
      "Minibatch perplexity: 30.03\n",
      "Validation set perplexity: 21.66\n",
      "Average loss at step 3000: 3.172941 learning rate: 10.000000\n",
      "Minibatch perplexity: 20.54\n",
      "Validation set perplexity: 21.98\n",
      "Average loss at step 3100: 3.153861 learning rate: 10.000000\n",
      "Minibatch perplexity: 21.34\n",
      "Validation set perplexity: 25.31\n",
      "Average loss at step 3200: 3.216171 learning rate: 10.000000\n",
      "Minibatch perplexity: 21.63\n",
      "Validation set perplexity: 20.35\n",
      "Average loss at step 3300: 3.237158 learning rate: 10.000000\n",
      "Minibatch perplexity: 26.47\n",
      "Validation set perplexity: 19.67\n",
      "Average loss at step 3400: 3.215141 learning rate: 10.000000\n",
      "Minibatch perplexity: 20.90\n",
      "Validation set perplexity: 20.02\n",
      "Average loss at step 3500: 3.141931 learning rate: 10.000000\n",
      "Minibatch perplexity: 22.34\n",
      "Validation set perplexity: 18.28\n",
      "Average loss at step 3600: 3.131192 learning rate: 10.000000\n",
      "Minibatch perplexity: 17.53\n",
      "Validation set perplexity: 16.26\n",
      "Average loss at step 3700: 3.078076 learning rate: 10.000000\n",
      "Minibatch perplexity: 19.67\n",
      "Validation set perplexity: 17.72\n",
      "Average loss at step 3800: 3.052949 learning rate: 10.000000\n",
      "Minibatch perplexity: 20.76\n",
      "Validation set perplexity: 20.29\n",
      "Average loss at step 3900: 3.051665 learning rate: 10.000000\n",
      "Minibatch perplexity: 20.16\n",
      "Validation set perplexity: 18.56\n",
      "Average loss at step 4000: 3.141447 learning rate: 10.000000\n",
      "Minibatch perplexity: 27.65\n",
      "Validation set perplexity: 22.14\n",
      "Average loss at step 4100: 3.098654 learning rate: 10.000000\n",
      "Minibatch perplexity: 20.89\n",
      "Validation set perplexity: 22.23\n",
      "Average loss at step 4200: 3.110016 learning rate: 10.000000\n",
      "Minibatch perplexity: 29.57\n",
      "Validation set perplexity: 23.18\n",
      "Average loss at step 4300: 3.113079 learning rate: 10.000000\n",
      "Minibatch perplexity: 26.25\n",
      "Validation set perplexity: 17.23\n",
      "Average loss at step 4400: 3.030991 learning rate: 10.000000\n",
      "Minibatch perplexity: 20.10\n",
      "Validation set perplexity: 21.91\n",
      "Average loss at step 4500: 3.036576 learning rate: 10.000000\n",
      "Minibatch perplexity: 25.72\n",
      "Validation set perplexity: 22.99\n",
      "Average loss at step 4600: 3.088558 learning rate: 10.000000\n",
      "Minibatch perplexity: 17.96\n",
      "Validation set perplexity: 22.89\n",
      "Average loss at step 4700: 3.095711 learning rate: 10.000000\n",
      "Minibatch perplexity: 26.07\n",
      "Validation set perplexity: 16.69\n",
      "Average loss at step 4800: 3.074759 learning rate: 10.000000\n",
      "Minibatch perplexity: 21.75\n",
      "Validation set perplexity: 17.65\n",
      "Average loss at step 4900: 3.110897 learning rate: 10.000000\n",
      "Minibatch perplexity: 24.59\n",
      "Validation set perplexity: 23.45\n",
      "Average loss at step 5000: 3.143002 learning rate: 1.000000\n",
      "Minibatch perplexity: 25.10\n",
      "Validation set perplexity: 20.47\n",
      "Average loss at step 5100: 3.076423 learning rate: 1.000000\n",
      "Minibatch perplexity: 23.38\n",
      "Validation set perplexity: 22.98\n",
      "Average loss at step 5200: 3.036379 learning rate: 1.000000\n",
      "Minibatch perplexity: 26.72\n",
      "Validation set perplexity: 22.36\n",
      "Average loss at step 5300: 3.097054 learning rate: 1.000000\n",
      "Minibatch perplexity: 22.36\n",
      "Validation set perplexity: 15.64\n",
      "Average loss at step 5400: 3.100626 learning rate: 1.000000\n",
      "Minibatch perplexity: 17.30\n",
      "Validation set perplexity: 13.26\n",
      "Average loss at step 5500: 3.087094 learning rate: 1.000000\n",
      "Minibatch perplexity: 23.92\n",
      "Validation set perplexity: 17.41\n",
      "Average loss at step 5600: 3.044852 learning rate: 1.000000\n",
      "Minibatch perplexity: 22.17\n",
      "Validation set perplexity: 23.37\n",
      "Average loss at step 5700: 3.068944 learning rate: 1.000000\n",
      "Minibatch perplexity: 22.63\n",
      "Validation set perplexity: 32.58\n",
      "Average loss at step 5800: 3.067244 learning rate: 1.000000\n",
      "Minibatch perplexity: 16.59\n",
      "Validation set perplexity: 32.17\n",
      "Average loss at step 5900: 3.026075 learning rate: 1.000000\n",
      "Minibatch perplexity: 21.06\n",
      "Validation set perplexity: 31.37\n",
      "Average loss at step 6000: 3.060883 learning rate: 1.000000\n",
      "Minibatch perplexity: 21.64\n",
      "Validation set perplexity: 28.73\n",
      "Average loss at step 6100: 3.033499 learning rate: 1.000000\n",
      "Minibatch perplexity: 25.96\n",
      "Validation set perplexity: 25.95\n",
      "Average loss at step 6200: 3.013863 learning rate: 1.000000\n",
      "Minibatch perplexity: 18.76\n",
      "Validation set perplexity: 31.16\n",
      "Average loss at step 6300: 3.011060 learning rate: 1.000000\n",
      "Minibatch perplexity: 16.33\n",
      "Validation set perplexity: 22.18\n",
      "Average loss at step 6400: 2.979777 learning rate: 1.000000\n",
      "Minibatch perplexity: 16.60\n",
      "Validation set perplexity: 36.04\n",
      "Average loss at step 6500: 2.979953 learning rate: 1.000000\n",
      "Minibatch perplexity: 19.25\n",
      "Validation set perplexity: 17.35\n",
      "Average loss at step 6600: 2.981188 learning rate: 1.000000\n",
      "Minibatch perplexity: 17.05\n",
      "Validation set perplexity: 15.43\n",
      "Average loss at step 6700: 3.063950 learning rate: 1.000000\n",
      "Minibatch perplexity: 14.26\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation set perplexity: 18.17\n",
      "Average loss at step 6800: 3.016259 learning rate: 1.000000\n",
      "Minibatch perplexity: 22.80\n",
      "Validation set perplexity: 20.05\n",
      "Average loss at step 6900: 3.016923 learning rate: 1.000000\n",
      "Minibatch perplexity: 19.66\n",
      "Validation set perplexity: 22.33\n",
      "Average loss at step 7000: 3.031210 learning rate: 1.000000\n",
      "Minibatch perplexity: 24.93\n",
      "Validation set perplexity: 18.10\n",
      "Average loss at step 7100: 3.045881 learning rate: 1.000000\n",
      "Minibatch perplexity: 20.39\n",
      "Validation set perplexity: 21.94\n",
      "Average loss at step 7200: 3.028909 learning rate: 1.000000\n",
      "Minibatch perplexity: 19.46\n",
      "Validation set perplexity: 18.83\n",
      "Average loss at step 7300: 3.061050 learning rate: 1.000000\n",
      "Minibatch perplexity: 22.89\n",
      "Validation set perplexity: 15.68\n",
      "Average loss at step 7400: 2.999575 learning rate: 1.000000\n",
      "Minibatch perplexity: 16.12\n",
      "Validation set perplexity: 14.52\n",
      "Average loss at step 7500: 2.962244 learning rate: 1.000000\n",
      "Minibatch perplexity: 19.69\n",
      "Validation set perplexity: 15.35\n",
      "Average loss at step 7600: 3.004800 learning rate: 1.000000\n",
      "Minibatch perplexity: 16.78\n",
      "Validation set perplexity: 16.00\n",
      "Average loss at step 7700: 3.031018 learning rate: 1.000000\n",
      "Minibatch perplexity: 19.66\n",
      "Validation set perplexity: 14.54\n",
      "Average loss at step 7800: 3.003736 learning rate: 1.000000\n",
      "Minibatch perplexity: 24.32\n",
      "Validation set perplexity: 16.86\n",
      "Average loss at step 7900: 3.045429 learning rate: 1.000000\n",
      "Minibatch perplexity: 13.49\n",
      "Validation set perplexity: 16.71\n",
      "Average loss at step 8000: 3.019979 learning rate: 1.000000\n",
      "Minibatch perplexity: 21.16\n",
      "Validation set perplexity: 17.29\n",
      "Average loss at step 8100: 3.011473 learning rate: 1.000000\n",
      "Minibatch perplexity: 16.98\n",
      "Validation set perplexity: 12.40\n",
      "Average loss at step 8200: 3.068742 learning rate: 1.000000\n",
      "Minibatch perplexity: 26.90\n",
      "Validation set perplexity: 19.03\n",
      "Average loss at step 8300: 3.047896 learning rate: 1.000000\n",
      "Minibatch perplexity: 21.03\n",
      "Validation set perplexity: 22.42\n",
      "Average loss at step 8400: 3.036801 learning rate: 1.000000\n",
      "Minibatch perplexity: 22.00\n",
      "Validation set perplexity: 20.92\n",
      "Average loss at step 8500: 2.999897 learning rate: 1.000000\n",
      "Minibatch perplexity: 14.46\n",
      "Validation set perplexity: 20.79\n",
      "Average loss at step 8600: 3.003514 learning rate: 1.000000\n",
      "Minibatch perplexity: 23.00\n",
      "Validation set perplexity: 18.11\n",
      "Average loss at step 8700: 3.005338 learning rate: 1.000000\n",
      "Minibatch perplexity: 17.73\n",
      "Validation set perplexity: 18.86\n",
      "Average loss at step 8800: 2.974102 learning rate: 1.000000\n",
      "Minibatch perplexity: 22.88\n",
      "Validation set perplexity: 21.26\n",
      "Average loss at step 8900: 2.999164 learning rate: 1.000000\n",
      "Minibatch perplexity: 15.43\n",
      "Validation set perplexity: 24.58\n",
      "Average loss at step 9000: 2.959185 learning rate: 1.000000\n",
      "Minibatch perplexity: 15.11\n",
      "Validation set perplexity: 16.94\n",
      "Average loss at step 9100: 2.944175 learning rate: 1.000000\n",
      "Minibatch perplexity: 18.70\n",
      "Validation set perplexity: 16.95\n",
      "Average loss at step 9200: 3.021137 learning rate: 1.000000\n",
      "Minibatch perplexity: 24.09\n",
      "Validation set perplexity: 17.99\n",
      "Average loss at step 9300: 3.065589 learning rate: 1.000000\n",
      "Minibatch perplexity: 21.57\n",
      "Validation set perplexity: 20.24\n",
      "Average loss at step 9400: 2.994427 learning rate: 1.000000\n",
      "Minibatch perplexity: 19.88\n",
      "Validation set perplexity: 14.94\n",
      "Average loss at step 9500: 2.977173 learning rate: 1.000000\n",
      "Minibatch perplexity: 17.70\n",
      "Validation set perplexity: 16.00\n",
      "Average loss at step 9600: 3.061991 learning rate: 1.000000\n",
      "Minibatch perplexity: 15.05\n",
      "Validation set perplexity: 17.59\n",
      "Average loss at step 9700: 3.004271 learning rate: 1.000000\n",
      "Minibatch perplexity: 19.39\n",
      "Validation set perplexity: 15.55\n",
      "Average loss at step 9800: 3.042983 learning rate: 1.000000\n",
      "Minibatch perplexity: 20.40\n",
      "Validation set perplexity: 13.01\n",
      "Average loss at step 9900: 2.911204 learning rate: 1.000000\n",
      "Minibatch perplexity: 17.42\n",
      "Validation set perplexity: 17.57\n",
      "Average loss at step 10000: 2.960470 learning rate: 0.100000\n",
      "Minibatch perplexity: 19.74\n",
      "Validation set perplexity: 21.98\n"
     ]
    }
   ],
   "source": [
    "num_steps = 10001\n",
    "summary_frequency = 100\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.initialize_all_variables().run()\n",
    "    print (\"Initialized\")\n",
    "    train_batches = BigramBatchGenerator(train_text, batch_size, num_unrollings)\n",
    "    valid_batches = BigramBatchGenerator(train_text, 1, 1)\n",
    "    mean_loss = 0\n",
    "    for step in range(num_steps):\n",
    "        batches = train_batches.next()\n",
    "        feed_dict = dict()\n",
    "        for i in range(num_unrollings + 1):\n",
    "            feed_dict[train_data[i]] = batches[i]\n",
    "        feed_dict[keep_prob] = 0.6\n",
    "        _, curr_loss, predictions, lr = session.run(\n",
    "            [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "        mean_loss += curr_loss\n",
    "        if step % summary_frequency == 0:\n",
    "            if step > 0:\n",
    "                mean_loss = mean_loss / summary_frequency\n",
    "                print('Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "            mean_loss = 0\n",
    "            labels = list(batches)[1:]\n",
    "            labels = np.concatenate([bigram2onehot(l) for l in labels])\n",
    "            print('Minibatch perplexity: %.2f' % float(np.exp(logprob(predictions, labels))))\n",
    "            # measure validation set perplexity\n",
    "            reset_sample_state.run()\n",
    "            valid_logprob = 0\n",
    "            for _ in range(valid_size):\n",
    "                b = valid_batches.next()\n",
    "                valid_predictions = sample_prediction.eval({sample_input: b[0], keep_prob: 1.0})\n",
    "                valid_labels = bigram2onehot(b[1])[0].reshape(1, -1)\n",
    "                valid_logprob = valid_logprob + logprob(valid_predictions, valid_labels)\n",
    "            print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "                valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Y5tapX3kpcqZ"
   },
   "source": [
    "---\n",
    "Problem 3\n",
    "---------\n",
    "\n",
    "(difficult!)\n",
    "\n",
    "Write a sequence-to-sequence LSTM which mirrors all the words in a sentence. For example, if your input is:\n",
    "\n",
    "    the quick brown fox\n",
    "    \n",
    "the model should attempt to output:\n",
    "\n",
    "    eht kciuq nworb xof\n",
    "    \n",
    "Refer to the lecture on how to put together a sequence-to-sequence model, as well as [this article](http://arxiv.org/abs/1409.3215) for best practices.\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "6_lstm.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
