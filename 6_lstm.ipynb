{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8tQJd2YSCfWR"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D7tqLMoKF6uq"
   },
   "source": [
    "Deep Learning\n",
    "=============\n",
    "\n",
    "Assignment 6\n",
    "------------\n",
    "\n",
    "After training a skip-gram model in `5_word2vec.ipynb`, the goal of this notebook is to train a LSTM character model over [Text8](http://mattmahoney.net/dc/textdata) data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "MvEblsgEXxrd"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import string\n",
    "import tensorflow as tf\n",
    "import zipfile\n",
    "from six.moves import range\n",
    "from six.moves.urllib.request import urlretrieve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 5993,
     "status": "ok",
     "timestamp": 1445965582896,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "RJ-o3UBUFtCw",
    "outputId": "d530534e-0791-4a94-ca6d-1c8f1b908a9e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found and verified text8.zip\n"
     ]
    }
   ],
   "source": [
    "url = 'http://mattmahoney.net/dc/'\n",
    "\n",
    "def maybe_download(filename, expected_bytes):\n",
    "  \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n",
    "  if not os.path.exists(filename):\n",
    "    filename, _ = urlretrieve(url + filename, filename)\n",
    "  statinfo = os.stat(filename)\n",
    "  if statinfo.st_size == expected_bytes:\n",
    "    print('Found and verified %s' % filename)\n",
    "  else:\n",
    "    print(statinfo.st_size)\n",
    "    raise Exception(\n",
    "      'Failed to verify ' + filename + '. Can you get to it with a browser?')\n",
    "  return filename\n",
    "\n",
    "filename = maybe_download('text8.zip', 31344016)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 5982,
     "status": "ok",
     "timestamp": 1445965582916,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "Mvf09fjugFU_",
    "outputId": "8f75db58-3862-404b-a0c3-799380597390"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size 100000000\n"
     ]
    }
   ],
   "source": [
    "def read_data(filename):\n",
    "  with zipfile.ZipFile(filename) as f:\n",
    "    name = f.namelist()[0]\n",
    "    data = tf.compat.as_str(f.read(name))\n",
    "  return data\n",
    "  \n",
    "text = read_data(filename)\n",
    "print('Data size %d' % len(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ga2CYACE-ghb"
   },
   "source": [
    "Create a small validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 6184,
     "status": "ok",
     "timestamp": 1445965583138,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "w-oBpfFG-j43",
    "outputId": "bdb96002-d021-4379-f6de-a977924f0d02"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99999000 ons anarchists advocate social relations based upon voluntary as\n",
      "1000  anarchism originated as a term of abuse first used against earl\n"
     ]
    }
   ],
   "source": [
    "valid_size = 1000\n",
    "valid_text = text[:valid_size]\n",
    "train_text = text[valid_size:]\n",
    "train_size = len(train_text)\n",
    "print(train_size, train_text[:64])\n",
    "print(valid_size, valid_text[:64])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Zdw6i4F8glpp"
   },
   "source": [
    "Utility functions to map characters to vocabulary IDs and back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 6276,
     "status": "ok",
     "timestamp": 1445965583249,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "gAL1EECXeZsD",
    "outputId": "88fc9032-feb9-45ff-a9a0-a26759cc1f2e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected character: ï\n",
      "1 26 0 0\n",
      "a z  \n"
     ]
    }
   ],
   "source": [
    "vocabulary_size = len(string.ascii_lowercase) + 1 # [a-z] + ' '\n",
    "first_letter = ord(string.ascii_lowercase[0])\n",
    "\n",
    "def char2id(char):\n",
    "  if char in string.ascii_lowercase:\n",
    "    return ord(char) - first_letter + 1\n",
    "  elif char == ' ':\n",
    "    return 0\n",
    "  else:\n",
    "    print('Unexpected character: %s' % char)\n",
    "    return 0\n",
    "  \n",
    "def id2char(dictid):\n",
    "  if dictid > 0:\n",
    "    return chr(dictid + first_letter - 1)\n",
    "  else:\n",
    "    return ' '\n",
    "\n",
    "print(char2id('a'), char2id('z'), char2id(' '), char2id('ï'))\n",
    "print(id2char(1), id2char(26), id2char(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lFwoyygOmWsL"
   },
   "source": [
    "Function to generate a training batch for the LSTM model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 6473,
     "status": "ok",
     "timestamp": 1445965583467,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "d9wMtjy5hCj9",
    "outputId": "3dd79c80-454a-4be0-8b71-4a4a357b3367"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ons anarchi', 'when milita', 'lleria arch', ' abbeys and', 'married urr', 'hel and ric', 'y and litur', 'ay opened f', 'tion from t', 'migration t', 'new york ot', 'he boeing s', 'e listed wi', 'eber has pr', 'o be made t', 'yer who rec', 'ore signifi', 'a fierce cr', ' two six ei', 'aristotle s', 'ity can be ', ' and intrac', 'tion of the', 'dy to pass ', 'f certain d', 'at it will ', 'e convince ', 'ent told hi', 'ampaign and', 'rver side s', 'ious texts ', 'o capitaliz', 'a duplicate', 'gh ann es d', 'ine january', 'ross zero t', 'cal theorie', 'ast instanc', ' dimensiona', 'most holy m', 't s support', 'u is still ', 'e oscillati', 'o eight sub', 'of italy la', 's the tower', 'klahoma pre', 'erprise lin', 'ws becomes ', 'et in a naz', 'the fabian ', 'etchy to re', ' sharman ne', 'ised empero', 'ting in pol', 'd neo latin', 'th risky ri', 'encyclopedi', 'fense the a', 'duating fro', 'treet grid ', 'ations more', 'appeal of d', 'si have mad']\n",
      "['ists advoca', 'ary governm', 'hes nationa', 'd monasteri', 'raca prince', 'chard baer ', 'rgical lang', 'for passeng', 'the nationa', 'took place ', 'ther well k', 'seven six s', 'ith a gloss', 'robably bee', 'to recogniz', 'ceived the ', 'icant than ', 'ritic of th', 'ight in sig', 's uncaused ', ' lost as in', 'cellular ic', 'e size of t', ' him a stic', 'drugs confu', ' take to co', ' the priest', 'im to name ', 'd barred at', 'standard fo', ' such as es', 'ze on the g', 'e of the or', 'd hiver one', 'y eight mar', 'the lead ch', 'es classica', 'ce the non ', 'al analysis', 'mormons bel', 't or at lea', ' disagreed ', 'ing system ', 'btypes base', 'anguages th', 'r commissio', 'ess one nin', 'nux suse li', ' the first ', 'zi concentr', ' society ne', 'elatively s', 'etworks sha', 'or hirohito', 'litical ini', 'n most of t', 'iskerdoo ri', 'ic overview', 'air compone', 'om acnm acc', ' centerline', 'e than any ', 'devotional ', 'de such dev']\n",
      "[' a']\n",
      "['an']\n"
     ]
    }
   ],
   "source": [
    "batch_size=64\n",
    "num_unrollings=10\n",
    "\n",
    "class BatchGenerator(object):\n",
    "  def __init__(self, text, batch_size, num_unrollings):\n",
    "    self._text = text\n",
    "    self._text_size = len(text)\n",
    "    self._batch_size = batch_size\n",
    "    self._num_unrollings = num_unrollings\n",
    "    segment = self._text_size // batch_size\n",
    "    self._cursor = [ offset * segment for offset in range(batch_size)]\n",
    "    self._last_batch = self._next_batch()\n",
    "  \n",
    "  def _next_batch(self):\n",
    "    \"\"\"Generate a single batch from the current cursor position in the data.\"\"\"\n",
    "    batch = np.zeros(shape=(self._batch_size, vocabulary_size), dtype=np.float)\n",
    "    for b in range(self._batch_size):\n",
    "      batch[b, char2id(self._text[self._cursor[b]])] = 1.0\n",
    "      self._cursor[b] = (self._cursor[b] + 1) % self._text_size\n",
    "    return batch\n",
    "  \n",
    "  def next(self):\n",
    "    \"\"\"Generate the next array of batches from the data. The array consists of\n",
    "    the last batch of the previous array, followed by num_unrollings new ones.\n",
    "    \"\"\"\n",
    "    batches = [self._last_batch]\n",
    "    for step in range(self._num_unrollings):\n",
    "      batches.append(self._next_batch())\n",
    "    self._last_batch = batches[-1]\n",
    "    return batches\n",
    "\n",
    "def characters(probabilities):\n",
    "  \"\"\"Turn a 1-hot encoding or a probability distribution over the possible\n",
    "  characters back into its (most likely) character representation.\"\"\"\n",
    "  return [id2char(c) for c in np.argmax(probabilities, 1)]\n",
    "\n",
    "def batches2string(batches):\n",
    "  \"\"\"Convert a sequence of batches back into their (most likely) string\n",
    "  representation.\"\"\"\n",
    "  s = [''] * batches[0].shape[0]\n",
    "  for b in batches:\n",
    "    s = [''.join(x) for x in zip(s, characters(b))]\n",
    "  return s\n",
    "\n",
    "train_batches = BatchGenerator(train_text, batch_size, num_unrollings)\n",
    "valid_batches = BatchGenerator(valid_text, 1, 1)\n",
    "\n",
    "print(batches2string(train_batches.next()))\n",
    "print(batches2string(train_batches.next()))\n",
    "print(batches2string(valid_batches.next()))\n",
    "print(batches2string(valid_batches.next()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "KyVd8FxT5QBc"
   },
   "outputs": [],
   "source": [
    "def logprob(predictions, labels):\n",
    "  \"\"\"Log-probability of the true labels in a predicted batch.\"\"\"\n",
    "  predictions[predictions < 1e-10] = 1e-10\n",
    "  return np.sum(np.multiply(labels, -np.log(predictions))) / labels.shape[0]\n",
    "\n",
    "def sample_distribution(distribution):\n",
    "  \"\"\"Sample one element from a distribution assumed to be an array of normalized\n",
    "  probabilities.\n",
    "  \"\"\"\n",
    "  r = random.uniform(0, 1)\n",
    "  s = 0\n",
    "  for i in range(len(distribution)):\n",
    "    s += distribution[i]\n",
    "    if s >= r:\n",
    "      return i\n",
    "  return len(distribution) - 1\n",
    "\n",
    "def sample(prediction):\n",
    "  \"\"\"Turn a (column) prediction into 1-hot encoded samples.\"\"\"\n",
    "  p = np.zeros(shape=[1, vocabulary_size], dtype=np.float)\n",
    "  p[0, sample_distribution(prediction[0])] = 1.0\n",
    "  return p\n",
    "\n",
    "def random_distribution():\n",
    "  \"\"\"Generate a random column of probabilities.\"\"\"\n",
    "  b = np.random.uniform(0.0, 1.0, size=[1, vocabulary_size])\n",
    "  return b/np.sum(b, 1)[:,None]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "K8f67YXaDr4C"
   },
   "source": [
    "Simple LSTM Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "Q5rxZK6RDuGe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-8-6eae96a73cce>:65: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See @{tf.nn.softmax_cross_entropy_with_logits_v2}.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "num_nodes = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "  # Parameters:\n",
    "  # Input gate: input, previous output, and bias.\n",
    "  ix = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  im = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ib = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Forget gate: input, previous output, and bias.\n",
    "  fx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  fm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  fb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Memory cell: input, state and bias.                             \n",
    "  cx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  cm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  cb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Output gate: input, previous output, and bias.\n",
    "  ox = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  om = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ob = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Variables saving state across unrollings.\n",
    "  saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  # Classifier weights and biases.\n",
    "  w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "  b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "  \n",
    "  # Definition of the cell computation.\n",
    "  def lstm_cell(i, o, state):\n",
    "    \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "    Note that in this formulation, we omit the various connections between the\n",
    "    previous state and the gates.\"\"\"\n",
    "    input_gate = tf.sigmoid(tf.matmul(i, ix) + tf.matmul(o, im) + ib)\n",
    "    forget_gate = tf.sigmoid(tf.matmul(i, fx) + tf.matmul(o, fm) + fb)\n",
    "    update = tf.matmul(i, cx) + tf.matmul(o, cm) + cb\n",
    "    state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "    output_gate = tf.sigmoid(tf.matmul(i, ox) + tf.matmul(o, om) + ob)\n",
    "    return output_gate * tf.tanh(state), state\n",
    "\n",
    "  # Input data.\n",
    "  train_data = list()\n",
    "  for _ in range(num_unrollings + 1):\n",
    "    train_data.append(\n",
    "      tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))\n",
    "  train_inputs = train_data[:num_unrollings]\n",
    "  train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
    "\n",
    "  # Unrolled LSTM loop.\n",
    "  outputs = list()\n",
    "  output = saved_output\n",
    "  state = saved_state\n",
    "  for i in train_inputs:\n",
    "    output, state = lstm_cell(i, output, state)\n",
    "    outputs.append(output)\n",
    "\n",
    "  # State saving across unrollings.\n",
    "  with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "    # Classifier.\n",
    "    logits = tf.nn.xw_plus_b(tf.concat(outputs, 0), w, b)\n",
    "    loss = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(\n",
    "        labels=tf.concat(train_labels, 0), logits=logits))\n",
    "\n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(\n",
    "    10.0, global_step, 5000, 0.1, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "  gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "  optimizer = optimizer.apply_gradients(\n",
    "    zip(gradients, v), global_step=global_step)\n",
    "\n",
    "  # Predictions.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "  # Sampling and validation eval: batch 1, no unrolling.\n",
    "  sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "  saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  reset_sample_state = tf.group(\n",
    "    saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "    saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "  sample_output, sample_state = lstm_cell(\n",
    "    sample_input, saved_sample_output, saved_sample_state)\n",
    "  with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 41
      },
      {
       "item_id": 80
      },
      {
       "item_id": 126
      },
      {
       "item_id": 144
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 199909,
     "status": "ok",
     "timestamp": 1445965877333,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "RD9zQCZTEaEm",
    "outputId": "5e868466-2532-4545-ce35-b403cf5d9de6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 3.294241 learning rate: 10.000000\n",
      "Minibatch perplexity: 26.96\n",
      "================================================================================\n",
      "kh ass xcdvdal a ebqltout  x eol eczushlledtx cp zi a c  hmvboaeiu fwfmr lqeieye\n",
      "yeujl vavvraphba att ley ngcaknmejiziziaigbwmdknniurkctddpstaaadffq v i ogn t c \n",
      "dhlooffc wxhhfp  er olwmimbdrps chwhbgfenweffflar gaw  moehv  na  kgnsomubgzn ya\n",
      "cxopxoqi  srjrdmmyfee w b atpen kkxhbrgunse vl  iaoisioiiixsp rpi rhbebjjmkcbnlt\n",
      "wksocpj yuotbp fe rqdiiwioktihltmmfvpbd  ig dxgn gdo dtephdkm xgui lb  f tckkbcx\n",
      "================================================================================\n",
      "Validation set perplexity: 20.23\n",
      "Average loss at step 100: 2.589402 learning rate: 10.000000\n",
      "Minibatch perplexity: 10.92\n",
      "Validation set perplexity: 10.42\n",
      "Average loss at step 200: 2.245768 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.54\n",
      "Validation set perplexity: 8.46\n",
      "Average loss at step 300: 2.096586 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.55\n",
      "Validation set perplexity: 8.07\n",
      "Average loss at step 400: 1.999306 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.46\n",
      "Validation set perplexity: 7.53\n",
      "Average loss at step 500: 1.933767 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.49\n",
      "Validation set perplexity: 6.93\n",
      "Average loss at step 600: 1.911262 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.28\n",
      "Validation set perplexity: 6.92\n",
      "Average loss at step 700: 1.860855 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.18\n",
      "Validation set perplexity: 6.38\n",
      "Average loss at step 800: 1.821417 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.10\n",
      "Validation set perplexity: 6.24\n",
      "Average loss at step 900: 1.835847 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.01\n",
      "Validation set perplexity: 6.08\n",
      "Average loss at step 1000: 1.829198 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.62\n",
      "================================================================================\n",
      " one five four two one one nine nine earocy fige the wis sustohi muxseeric econo\n",
      "kiteg searg cormper of the cluterge fiplaye natly penfocement deaned to ceme whi\n",
      "fincter pirmed the colply the byowime persomofly of be ffoolm one nine five le p\n",
      "d us is lew shawable acom the s enolons hes moye aloud buing new of the four sou\n",
      "s from speces evenown and appold to his idlinder of provace the distix resicies \n",
      "================================================================================\n",
      "Validation set perplexity: 5.96\n",
      "Average loss at step 1100: 1.782048 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.59\n",
      "Validation set perplexity: 5.80\n",
      "Average loss at step 1200: 1.757666 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.12\n",
      "Validation set perplexity: 5.53\n",
      "Average loss at step 1300: 1.740146 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.77\n",
      "Validation set perplexity: 5.48\n",
      "Average loss at step 1400: 1.749252 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.90\n",
      "Validation set perplexity: 5.43\n",
      "Average loss at step 1500: 1.738887 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.79\n",
      "Validation set perplexity: 5.39\n",
      "Average loss at step 1600: 1.749289 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.47\n",
      "Validation set perplexity: 5.47\n",
      "Average loss at step 1700: 1.719031 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.67\n",
      "Validation set perplexity: 5.43\n",
      "Average loss at step 1800: 1.676301 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.53\n",
      "Validation set perplexity: 5.23\n",
      "Average loss at step 1900: 1.651177 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.10\n",
      "Validation set perplexity: 5.15\n",
      "Average loss at step 2000: 1.697199 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.84\n",
      "================================================================================\n",
      "neral whore jomitancians of the parts klesh s not is requence on ameing wosed it\n",
      "cor on solicime and hour to text its took setsing own the ajonn jotywom sioning \n",
      "tivis twu comeding to maser to the applans the d is companiain transliln to comp\n",
      "des diffines to imanuing steries is poningy c or litient selopnived yanoses tith\n",
      "worll number a bomme soopharitword toughy strofts ilsondoagy work parlies vorica\n",
      "================================================================================\n",
      "Validation set perplexity: 5.11\n",
      "Average loss at step 2100: 1.685013 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.13\n",
      "Validation set perplexity: 4.87\n",
      "Average loss at step 2200: 1.683843 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.40\n",
      "Validation set perplexity: 5.02\n",
      "Average loss at step 2300: 1.645529 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.14\n",
      "Validation set perplexity: 4.74\n",
      "Average loss at step 2400: 1.660319 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.18\n",
      "Validation set perplexity: 4.78\n",
      "Average loss at step 2500: 1.684385 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.35\n",
      "Validation set perplexity: 4.67\n",
      "Average loss at step 2600: 1.659191 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.91\n",
      "Validation set perplexity: 4.58\n",
      "Average loss at step 2700: 1.661124 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.59\n",
      "Validation set perplexity: 4.70\n",
      "Average loss at step 2800: 1.654391 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.56\n",
      "Validation set perplexity: 4.67\n",
      "Average loss at step 2900: 1.655797 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.84\n",
      "Validation set perplexity: 4.69\n",
      "Average loss at step 3000: 1.658391 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.02\n",
      "================================================================================\n",
      "wasp eey shoted poands siteria was of partity schosion civided wift be themean s\n",
      "mall betcotsoliginess to particated film contrublillular marning zatty dericing \n",
      "warts one airso hiscome in himperso indically unrerrafron with resiscipabcradetf\n",
      "crouns has postlion whe schoticish wrate and maryly spamacrater sain sands is po\n",
      "an humbima cospirsed comsunited rematisally zero sevis sevtioned with and underm\n",
      "================================================================================\n",
      "Validation set perplexity: 4.59\n",
      "Average loss at step 3100: 1.633512 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.81\n",
      "Validation set perplexity: 4.54\n",
      "Average loss at step 3200: 1.649096 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.62\n",
      "Validation set perplexity: 4.54\n",
      "Average loss at step 3300: 1.640538 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.02\n",
      "Validation set perplexity: 4.65\n",
      "Average loss at step 3400: 1.678512 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.49\n",
      "Validation set perplexity: 4.60\n",
      "Average loss at step 3500: 1.662755 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.63\n",
      "Validation set perplexity: 4.64\n",
      "Average loss at step 3600: 1.670951 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.46\n",
      "Validation set perplexity: 4.65\n",
      "Average loss at step 3700: 1.649719 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.08\n",
      "Validation set perplexity: 4.55\n",
      "Average loss at step 3800: 1.645336 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.54\n",
      "Validation set perplexity: 4.73\n",
      "Average loss at step 3900: 1.638620 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.30\n",
      "Validation set perplexity: 4.62\n",
      "Average loss at step 4000: 1.656294 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.86\n",
      "================================================================================\n",
      "tredier scholams onewninger seatherms indodian ind these sequal effounual lowing\n",
      "hilying in the phordancges of augus ideact only muster withild in the rockens ro\n",
      "m embordy wede free he songer a nation ordan games fas sears publive to germaniu\n",
      "it anj when befores neypon helitmaka destind one two twentreas ored be for many \n",
      "red of and mauafk eallemon one auttrus et lebset ims phoders or tempt kide be mi\n",
      "================================================================================\n",
      "Validation set perplexity: 4.66\n",
      "Average loss at step 4100: 1.634725 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.31\n",
      "Validation set perplexity: 4.73\n",
      "Average loss at step 4200: 1.642699 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.17\n",
      "Validation set perplexity: 4.53\n",
      "Average loss at step 4300: 1.614943 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.10\n",
      "Validation set perplexity: 4.57\n",
      "Average loss at step 4400: 1.613606 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.88\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation set perplexity: 4.41\n",
      "Average loss at step 4500: 1.617777 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.16\n",
      "Validation set perplexity: 4.53\n",
      "Average loss at step 4600: 1.616890 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.88\n",
      "Validation set perplexity: 4.57\n",
      "Average loss at step 4700: 1.626374 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.28\n",
      "Validation set perplexity: 4.57\n",
      "Average loss at step 4800: 1.632236 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.31\n",
      "Validation set perplexity: 4.56\n",
      "Average loss at step 4900: 1.632752 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.31\n",
      "Validation set perplexity: 4.64\n",
      "Average loss at step 5000: 1.607245 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.71\n",
      "================================================================================\n",
      "urly probruaminer se what liberative the grans destlede we death zeake gred desu\n",
      "paul kive betwinnall idale zenoa roblin economit pocidot such them one siversion\n",
      "d mod vista cithold appant tran schoptical by deside a characties tu are amberge\n",
      "en is songea caven the velveet impexing two zero nine sumerakivary traves sitera\n",
      "violed one nine emyel two four digillabliber transhage lome two zero zero zero z\n",
      "================================================================================\n",
      "Validation set perplexity: 4.69\n",
      "Average loss at step 5100: 1.608096 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.95\n",
      "Validation set perplexity: 4.51\n",
      "Average loss at step 5200: 1.595405 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.75\n",
      "Validation set perplexity: 4.44\n",
      "Average loss at step 5300: 1.581044 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.60\n",
      "Validation set perplexity: 4.42\n",
      "Average loss at step 5400: 1.580622 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.21\n",
      "Validation set perplexity: 4.40\n",
      "Average loss at step 5500: 1.569709 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.92\n",
      "Validation set perplexity: 4.35\n",
      "Average loss at step 5600: 1.581908 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.99\n",
      "Validation set perplexity: 4.35\n",
      "Average loss at step 5700: 1.571681 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.49\n",
      "Validation set perplexity: 4.36\n",
      "Average loss at step 5800: 1.578483 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.86\n",
      "Validation set perplexity: 4.35\n",
      "Average loss at step 5900: 1.576065 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.06\n",
      "Validation set perplexity: 4.33\n",
      "Average loss at step 6000: 1.549099 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.09\n",
      "================================================================================\n",
      "er seakure main one nine simpts in one and one nine nine twreinged by can ghanw \n",
      "goys to the charles judely of the some razi posust gos the classics by beginion \n",
      " of a still larges known by beased more f reginal all crusternce in others are a\n",
      "per kel fails not law searmench tover gostor was was b releaside its langedifica\n",
      "jegt of meginor who scheld placy can discrofficate with one recorrs they of the \n",
      "================================================================================\n",
      "Validation set perplexity: 4.34\n",
      "Average loss at step 6100: 1.569263 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.15\n",
      "Validation set perplexity: 4.32\n",
      "Average loss at step 6200: 1.539957 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.75\n",
      "Validation set perplexity: 4.31\n",
      "Average loss at step 6300: 1.546957 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.94\n",
      "Validation set perplexity: 4.28\n",
      "Average loss at step 6400: 1.542059 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.68\n",
      "Validation set perplexity: 4.30\n",
      "Average loss at step 6500: 1.558015 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.84\n",
      "Validation set perplexity: 4.30\n",
      "Average loss at step 6600: 1.599075 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.72\n",
      "Validation set perplexity: 4.30\n",
      "Average loss at step 6700: 1.582624 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.24\n",
      "Validation set perplexity: 4.34\n",
      "Average loss at step 6800: 1.604544 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.86\n",
      "Validation set perplexity: 4.29\n",
      "Average loss at step 6900: 1.581612 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.68\n",
      "Validation set perplexity: 4.30\n",
      "Average loss at step 7000: 1.579660 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.97\n",
      "================================================================================\n",
      "nations shu freea still of some accers of d haups listary others espqy he hares \n",
      "e vowand d one aida from the beught onl of jolivily meet the more a moorle two z\n",
      "ory to freeline it scug on gerion and observers echadags theoes ost is the borsh\n",
      "olds have rathom of these since nineence scole one de no moreger with than one n\n",
      "nation sixper country their procilly oy spanistals bewnethousk winnatic dia worl\n",
      "================================================================================\n",
      "Validation set perplexity: 4.28\n"
     ]
    }
   ],
   "source": [
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print('Initialized')\n",
    "  mean_loss = 0\n",
    "  for step in range(num_steps):\n",
    "    batches = train_batches.next()\n",
    "    feed_dict = dict()\n",
    "    for i in range(num_unrollings + 1):\n",
    "      feed_dict[train_data[i]] = batches[i] # feed each placeholder in train_data\n",
    "    _, l, predictions, lr = session.run(\n",
    "      [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "    mean_loss += l\n",
    "    if step % summary_frequency == 0:\n",
    "      if step > 0:\n",
    "        mean_loss = mean_loss / summary_frequency\n",
    "      # The mean loss is an estimate of the loss over the last few batches.\n",
    "      print(\n",
    "        'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "      mean_loss = 0\n",
    "      labels = np.concatenate(list(batches)[1:])\n",
    "      print('Minibatch perplexity: %.2f' % float(\n",
    "        np.exp(logprob(predictions, labels))))\n",
    "      if step % (summary_frequency * 10) == 0:\n",
    "        # Generate some samples.\n",
    "        print('=' * 80)\n",
    "        for _ in range(5):\n",
    "          feed = sample(random_distribution())\n",
    "          sentence = characters(feed)[0]\n",
    "          reset_sample_state.run()\n",
    "          for _ in range(79):\n",
    "            prediction = sample_prediction.eval({sample_input: feed})\n",
    "            feed = sample(prediction)\n",
    "            sentence += characters(feed)[0]\n",
    "          print(sentence)\n",
    "        print('=' * 80)\n",
    "      # Measure validation set perplexity.\n",
    "      reset_sample_state.run()\n",
    "      valid_logprob = 0\n",
    "      for _ in range(valid_size):\n",
    "        b = valid_batches.next()\n",
    "        predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "        valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "      print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "        valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pl4vtmFfa5nn"
   },
   "source": [
    "---\n",
    "Problem 1\n",
    "---------\n",
    "\n",
    "You might have noticed that the definition of the LSTM cell involves 4 matrix multiplications with the input, and 4 matrix multiplications with the output. Simplify the expression by using a single matrix multiply for each, and variables that are 4 times larger.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_nodes = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "  # Parameters:\n",
    "  gx = tf.Variable(tf.truncated_normal([vocabulary_size, 4 * num_nodes], -0.1, 0.1))\n",
    "  gm = tf.Variable(tf.truncated_normal([num_nodes, 4 * num_nodes], -0.1, 0.1))\n",
    "  gb = tf.Variable(tf.zeros([1, 4 * num_nodes]))  \n",
    "  # Variables saving state across unrollings.\n",
    "  saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  # Classifier weights and biases.\n",
    "  w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "  b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "\n",
    "  # Definition of the cell computation.\n",
    "  def lstm_cell_v2 (i, o, state):\n",
    "    \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "    Note that in this formulation, we omit the various connections between the\n",
    "    previous state and the gates.\"\"\"\n",
    "#     input_gate = tf.sigmoid(tf.matmul(i, ix) + tf.matmul(o, im) + ib)\n",
    "#     forget_gate = tf.sigmoid(tf.matmul(i, fx) + tf.matmul(o, fm) + fb)\n",
    "#     update = tf.matmul(i, cx) + tf.matmul(o, cm) + cb\n",
    "#     state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "#     output_gate = tf.sigmoid(tf.matmul(i, ox) + tf.matmul(o, om) + ob)\n",
    "    gate = tf.matmul(i, gx) + tf.matmul(o, gm) + gb\n",
    "    input_gate = tf.sigmoid(gate[:, :num_nodes])\n",
    "    forget_gate = tf.sigmoid(gate[:, num_nodes:2*num_nodes])\n",
    "    update = gate[:, 2*num_nodes:3*num_nodes]\n",
    "    output_gate = tf.sigmoid(gate[:, 3*num_nodes:])\n",
    "    state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "    return output_gate * tf.tanh(state), state\n",
    "\n",
    "  # Input data.\n",
    "  train_data = list()\n",
    "  for _ in range(num_unrollings + 1):\n",
    "    train_data.append(\n",
    "      tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))\n",
    "  train_inputs = train_data[:num_unrollings]\n",
    "  train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
    "\n",
    "  # Unrolled LSTM loop.\n",
    "  outputs = list()\n",
    "  output = saved_output\n",
    "  state = saved_state\n",
    "  for i in train_inputs:\n",
    "    output, state = lstm_cell_v2(i, output, state)\n",
    "    outputs.append(output)\n",
    "\n",
    "  # State saving across unrollings.\n",
    "  with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "    # Classifier.\n",
    "    logits = tf.nn.xw_plus_b(tf.concat(outputs, 0), w, b)\n",
    "    loss = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(\n",
    "        labels=tf.concat(train_labels, 0), logits=logits))\n",
    "\n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(\n",
    "    10.0, global_step, 5000, 0.1, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "  gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "  optimizer = optimizer.apply_gradients(\n",
    "    zip(gradients, v), global_step=global_step)\n",
    "\n",
    "  # Predictions.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "  # Sampling and validation eval: batch 1, no unrolling.\n",
    "  sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "  saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  reset_sample_state = tf.group(\n",
    "    saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "    saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "  sample_output, sample_state = lstm_cell_v2(\n",
    "    sample_input, saved_sample_output, saved_sample_state)\n",
    "  with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 3.297089 learning rate: 10.000000\n",
      "Minibatch perplexity: 27.03\n",
      "================================================================================\n",
      "c  hg pe  emxkex abeqjente hnk  z aodl  ca  etesjgridsf aai mcsthzdqaft ehiotmm \n",
      "obcrpntfyxv muo e tyyx qn  mel ifeh db   ghcn okeva xhthnc  iao ewdibsgm hiyikst\n",
      "tcnfdvaazvdlqfjig v ew de w iws gfrbgiim pnbedhve sth  m uqesi ctlpvqb r  go m y\n",
      "tn cnnheashlfh ulheet yzi em fjxafraebvtexl rbbeeza eeg p odeswnet sy  kek vk   \n",
      "y d o vt    eleixbozsboeewwwfej  vtasna f rarevtgftfnhih gh mbnvibsndfz  drqsclk\n",
      "================================================================================\n",
      "Validation set perplexity: 20.02\n",
      "Average loss at step 100: 2.588254 learning rate: 10.000000\n",
      "Minibatch perplexity: 10.35\n",
      "Validation set perplexity: 10.36\n",
      "Average loss at step 200: 2.237018 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.18\n",
      "Validation set perplexity: 8.84\n",
      "Average loss at step 300: 2.082370 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.27\n",
      "Validation set perplexity: 8.09\n",
      "Average loss at step 400: 2.030543 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.83\n",
      "Validation set perplexity: 7.89\n",
      "Average loss at step 500: 1.975514 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.57\n",
      "Validation set perplexity: 7.10\n",
      "Average loss at step 600: 1.895978 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.44\n",
      "Validation set perplexity: 6.91\n",
      "Average loss at step 700: 1.872282 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.03\n",
      "Validation set perplexity: 6.56\n",
      "Average loss at step 800: 1.868920 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.24\n",
      "Validation set perplexity: 6.50\n",
      "Average loss at step 900: 1.844004 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.86\n",
      "Validation set perplexity: 6.47\n",
      "Average loss at step 1000: 1.846596 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.48\n",
      "================================================================================\n",
      "tred onlysory and on b wogka silobnotexment in chinis lay estancebyean dis noth \n",
      "jhy from wail nunch prentrationst hisoun recous whinecinghask finise reverones t\n",
      "y hpspazlibu baseds bogich backht to the ithound experioures on adowtherase form\n",
      "w at leman sommolicms one nine six med by sipenivate vunce moqued one one six fi\n",
      "acm of strres purde prateagre syvermencioual curpried by corised pork wil the mo\n",
      "================================================================================\n",
      "Validation set perplexity: 6.13\n",
      "Average loss at step 1100: 1.797940 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.50\n",
      "Validation set perplexity: 6.10\n",
      "Average loss at step 1200: 1.770989 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.22\n",
      "Validation set perplexity: 5.96\n",
      "Average loss at step 1300: 1.756063 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.85\n",
      "Validation set perplexity: 5.66\n",
      "Average loss at step 1400: 1.758939 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.91\n",
      "Validation set perplexity: 5.66\n",
      "Average loss at step 1500: 1.746530 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.62\n",
      "Validation set perplexity: 5.34\n",
      "Average loss at step 1600: 1.728987 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.53\n",
      "Validation set perplexity: 5.47\n",
      "Average loss at step 1700: 1.716712 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.23\n",
      "Validation set perplexity: 5.38\n",
      "Average loss at step 1800: 1.691207 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.05\n",
      "Validation set perplexity: 5.16\n",
      "Average loss at step 1900: 1.696352 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.23\n",
      "Validation set perplexity: 5.17\n",
      "Average loss at step 2000: 1.681787 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.23\n",
      "================================================================================\n",
      "vel planers about accles affiles that in times not to constion mudio insurces of\n",
      "x wives his i dega his douts on nurnees when he are two zero zero four mode tral\n",
      "ikn pesprophis ira proplunts fictives ismactuals one nine shorh carromils s s fu\n",
      "kated in later shespendever with bodanifore to the so compasence calartions seve\n",
      "k to consign and devernory destrabino inferranda braw do verseen dara to final l\n",
      "================================================================================\n",
      "Validation set perplexity: 5.31\n",
      "Average loss at step 2100: 1.693261 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.87\n",
      "Validation set perplexity: 5.18\n",
      "Average loss at step 2200: 1.710359 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.91\n",
      "Validation set perplexity: 5.07\n",
      "Average loss at step 2300: 1.712880 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.36\n",
      "Validation set perplexity: 5.00\n",
      "Average loss at step 2400: 1.686499 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.68\n",
      "Validation set perplexity: 5.07\n",
      "Average loss at step 2500: 1.695466 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.90\n",
      "Validation set perplexity: 5.10\n",
      "Average loss at step 2600: 1.675818 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.25\n",
      "Validation set perplexity: 4.90\n",
      "Average loss at step 2700: 1.686279 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.21\n",
      "Validation set perplexity: 4.87\n",
      "Average loss at step 2800: 1.685436 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.31\n",
      "Validation set perplexity: 5.01\n",
      "Average loss at step 2900: 1.679985 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.27\n",
      "Validation set perplexity: 5.02\n",
      "Average loss at step 3000: 1.685230 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.03\n",
      "================================================================================\n",
      "u legby wolk often is conames the imartick of buil ms seluphex clanes enjlimol c\n",
      "jar sested unite set the one nine three exclusions gede age was the keligh from \n",
      "jentia logab indepented the shrmandems to dite about kn ze of also south geit fr\n",
      " and taurs to the brea twe zero three one nine two zero nine ma firs in the frez\n",
      " air or resingtional cers clivation infectional elext known foid two of that the\n",
      "================================================================================\n",
      "Validation set perplexity: 4.87\n",
      "Average loss at step 3100: 1.656950 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.37\n",
      "Validation set perplexity: 4.82\n",
      "Average loss at step 3200: 1.643398 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.44\n",
      "Validation set perplexity: 4.72\n",
      "Average loss at step 3300: 1.651330 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.38\n",
      "Validation set perplexity: 4.72\n",
      "Average loss at step 3400: 1.640456 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.48\n",
      "Validation set perplexity: 4.77\n",
      "Average loss at step 3500: 1.678544 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.92\n",
      "Validation set perplexity: 4.81\n",
      "Average loss at step 3600: 1.655763 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.40\n",
      "Validation set perplexity: 4.73\n",
      "Average loss at step 3700: 1.659053 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.11\n",
      "Validation set perplexity: 4.82\n",
      "Average loss at step 3800: 1.662994 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.04\n",
      "Validation set perplexity: 4.78\n",
      "Average loss at step 3900: 1.657366 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.47\n",
      "Validation set perplexity: 4.71\n",
      "Average loss at step 4000: 1.644820 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.36\n",
      "================================================================================\n",
      "y caplandard batznuan all terved to the souther windors fame ele who clouned fir\n",
      "ent of the was one nine five eight four fistes see nobegapome ldsbist of begots \n",
      "grames of shas b hanstone gehosowal sh one nine four seven catters of selibac co\n",
      "z pragee prolates when one one one nationaly ase quands audiad is j a territophs\n",
      "ors medra the touts elected of these was the waths kirsted the gan rebilling the\n",
      "================================================================================\n",
      "Validation set perplexity: 4.63\n",
      "Average loss at step 4100: 1.623635 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.77\n",
      "Validation set perplexity: 4.65\n",
      "Average loss at step 4200: 1.619175 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.95\n",
      "Validation set perplexity: 4.65\n",
      "Average loss at step 4300: 1.623326 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.49\n",
      "Validation set perplexity: 4.74\n",
      "Average loss at step 4400: 1.611052 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.45\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation set perplexity: 4.66\n",
      "Average loss at step 4500: 1.642165 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.24\n",
      "Validation set perplexity: 4.72\n",
      "Average loss at step 4600: 1.628369 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.46\n",
      "Validation set perplexity: 4.73\n",
      "Average loss at step 4700: 1.625429 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.79\n",
      "Validation set perplexity: 4.83\n",
      "Average loss at step 4800: 1.610748 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.68\n",
      "Validation set perplexity: 4.78\n",
      "Average loss at step 4900: 1.622197 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.39\n",
      "Validation set perplexity: 4.75\n",
      "Average loss at step 5000: 1.614410 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.81\n",
      "================================================================================\n",
      "quaria used an allogia nigican the congss chailer accoptormensor was babum charl\n",
      "zative to been the the languay joad the sign in indetimal embered laf the report\n",
      "ous as the whuthainstity what from as dvanivate of not imperath the treatic fatu\n",
      "ch reals in peacisted the best town chribs to disprased twa convectic comporence\n",
      "qurazd dadrinia with the mayus two two thryes the pressity for alwereled emount \n",
      "================================================================================\n",
      "Validation set perplexity: 4.75\n",
      "Average loss at step 5100: 1.591449 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.19\n",
      "Validation set perplexity: 4.66\n",
      "Average loss at step 5200: 1.593782 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.22\n",
      "Validation set perplexity: 4.66\n",
      "Average loss at step 5300: 1.597679 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.24\n",
      "Validation set perplexity: 4.62\n",
      "Average loss at step 5400: 1.596142 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.71\n",
      "Validation set perplexity: 4.57\n",
      "Average loss at step 5500: 1.590832 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.33\n",
      "Validation set perplexity: 4.55\n",
      "Average loss at step 5600: 1.564737 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.31\n",
      "Validation set perplexity: 4.52\n",
      "Average loss at step 5700: 1.582015 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.70\n",
      "Validation set perplexity: 4.49\n",
      "Average loss at step 5800: 1.602042 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.59\n",
      "Validation set perplexity: 4.49\n",
      "Average loss at step 5900: 1.584564 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.17\n",
      "Validation set perplexity: 4.51\n",
      "Average loss at step 6000: 1.582072 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.76\n",
      "================================================================================\n",
      "f iraclic addrent it directively eight two the two s whole nine ital city s in t\n",
      "illy work and as in trusial untople divined s seven the booples oner the a new p\n",
      "lestix electence in ana roakes had knoururdi the risea was a sita that the lost \n",
      "s two concentrol barders are wind and respineted be the as parations of network \n",
      "s wede a datects tr american to neen vistch have arear a proscriments year rebie\n",
      "================================================================================\n",
      "Validation set perplexity: 4.46\n",
      "Average loss at step 6100: 1.577783 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.56\n",
      "Validation set perplexity: 4.51\n",
      "Average loss at step 6200: 1.591770 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.67\n",
      "Validation set perplexity: 4.53\n",
      "Average loss at step 6300: 1.591908 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.28\n",
      "Validation set perplexity: 4.56\n",
      "Average loss at step 6400: 1.574236 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.15\n",
      "Validation set perplexity: 4.54\n",
      "Average loss at step 6500: 1.558998 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.48\n",
      "Validation set perplexity: 4.58\n",
      "Average loss at step 6600: 1.600322 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.98\n",
      "Validation set perplexity: 4.57\n",
      "Average loss at step 6700: 1.571627 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.24\n",
      "Validation set perplexity: 4.52\n",
      "Average loss at step 6800: 1.573426 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.83\n",
      "Validation set perplexity: 4.56\n",
      "Average loss at step 6900: 1.573693 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.65\n",
      "Validation set perplexity: 4.50\n",
      "Average loss at step 7000: 1.591189 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.97\n",
      "================================================================================\n",
      "h one eight seven two two zero zero nine the southern and the with deploties sta\n",
      "ur abrused down peteraberia soputorism of contraccial and d undats becki wirds a\n",
      "ated invilmed line improreging bornmenfoyers war exploshus the olds foveen name \n",
      "ra of bass produce the governments chime event fele jing of eartharancan edgces \n",
      "ut not whice lake of smination of the uses under andivil where the data charle c\n",
      "================================================================================\n",
      "Validation set perplexity: 4.48\n"
     ]
    }
   ],
   "source": [
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print('Initialized')\n",
    "  mean_loss = 0\n",
    "  for step in range(num_steps):\n",
    "    batches = train_batches.next()\n",
    "    feed_dict = dict()\n",
    "    for i in range(num_unrollings + 1):\n",
    "      feed_dict[train_data[i]] = batches[i] # feed each placeholder in train_data\n",
    "    _, l, predictions, lr = session.run(\n",
    "      [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "    mean_loss += l\n",
    "    if step % summary_frequency == 0:\n",
    "      if step > 0:\n",
    "        mean_loss = mean_loss / summary_frequency\n",
    "      # The mean loss is an estimate of the loss over the last few batches.\n",
    "      print(\n",
    "        'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "      mean_loss = 0\n",
    "      labels = np.concatenate(list(batches)[1:])\n",
    "      print('Minibatch perplexity: %.2f' % float(\n",
    "        np.exp(logprob(predictions, labels))))\n",
    "      if step % (summary_frequency * 10) == 0:\n",
    "        # Generate some samples.\n",
    "        print('=' * 80)\n",
    "        for _ in range(5):\n",
    "          feed = sample(random_distribution())\n",
    "          sentence = characters(feed)[0]\n",
    "          reset_sample_state.run()\n",
    "          for _ in range(79):\n",
    "            prediction = sample_prediction.eval({sample_input: feed})\n",
    "            feed = sample(prediction)\n",
    "            sentence += characters(feed)[0]\n",
    "          print(sentence)\n",
    "        print('=' * 80)\n",
    "      # Measure validation set perplexity.\n",
    "      reset_sample_state.run()\n",
    "      valid_logprob = 0\n",
    "      for _ in range(valid_size):\n",
    "        b = valid_batches.next()\n",
    "        predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "        valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "      print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "        valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4eErTCTybtph"
   },
   "source": [
    "---\n",
    "Problem 2\n",
    "---------\n",
    "\n",
    "We want to train a LSTM over bigrams, that is pairs of consecutive characters like 'ab' instead of single characters like 'a'. Since the number of possible bigrams is large, feeding them directly to the LSTM using 1-hot encodings will lead to a very sparse representation that is very wasteful computationally.\n",
    "\n",
    "a- Introduce an embedding lookup on the inputs, and feed the embeddings to the LSTM cell instead of the inputs themselves.\n",
    "\n",
    "b- Write a bigram-based LSTM, modeled on the character LSTM above.\n",
    "\n",
    "c- Introduce Dropout. For best practices on how to use Dropout in LSTMs, refer to this [article](http://arxiv.org/abs/1409.2329).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary_size = len(string.ascii_lowercase) + 1 # [a-z] + ' '\n",
    "bigram_size = vocabulary_size * vocabulary_size\n",
    "\n",
    "class BigramBatchGenerator:\n",
    "    def __init__(self, text, batch_size, num_unrollings):\n",
    "        self._text = text\n",
    "        self._text_char_size = len(text)\n",
    "        self._text_bigram_size = len(text) // 2\n",
    "        self._batch_size = batch_size\n",
    "        self._num_unrollings = num_unrollings\n",
    "        segment = self._text_bigram_size // batch_size\n",
    "        self._cursor = [offset * segment for offset in range(batch_size)]\n",
    "        self._last_batch = self._next_batch()\n",
    "        \n",
    "    def _next_batch(self):\n",
    "        \"\"\"Generate a single batch from the current cursor position in the data.\"\"\"\n",
    "        batch = np.zeros(shape=self._batch_size, dtype=np.int32)\n",
    "        for b in range(self._batch_size):\n",
    "            char_idx = self._cursor[b] * 2\n",
    "            ch1 = char2id(self._text[char_idx])\n",
    "            ch2 = 0\n",
    "            if char_idx + 1 < self._text_char_size:\n",
    "                ch2 = char2id(self._text[char_idx + 1])\n",
    "            batch[b] = ch1 * vocabulary_size + ch2\n",
    "            self._cursor[b] = (self._cursor[b] + 1) % self._text_bigram_size\n",
    "        return batch\n",
    "    \n",
    "    def next(self):\n",
    "        batches = [self._last_batch]\n",
    "        for step in range(self._num_unrollings):\n",
    "            batches.append(self._next_batch())\n",
    "        self._last_batch = batches[-1]\n",
    "        return batches  \n",
    "\n",
    "onehot_map = np.zeros([bigram_size, bigram_size])\n",
    "np.fill_diagonal(onehot_map, 1)\n",
    "\n",
    "def bigram2onehot(encodings):\n",
    "    return [onehot_map[e] for e in encodings]\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_nodes = 512\n",
    "num_unrollings = 10\n",
    "batch_size = 32\n",
    "embedding_size = 128\n",
    "keep_prob = 0.6\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    # input to all gates\n",
    "    x = tf.Variable(tf.truncated_normal([embedding_size, num_nodes * 4], -0.1, 0.1))\n",
    "    # memory to all gates\n",
    "    m = tf.Variable(tf.truncated_normal([num_nodes, num_nodes * 4], -0.1, 0.1))\n",
    "    # biases to all gates\n",
    "    biases = tf.Variable(tf.zeros([1, num_nodes * 4]))\n",
    "    # Variables saving state across unrollings.\n",
    "    saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    # Classifier weights and biases.\n",
    "    w = tf.Variable(tf.truncated_normal([num_nodes, bigram_size], -0.1, 0.1))\n",
    "    b = tf.Variable(tf.zeros([bigram_size]))\n",
    "    # bigram embeddings\n",
    "    embeddings = tf.Variable(tf.truncated_normal([bigram_size, embedding_size], -0.1, 0.1))\n",
    "    # bigram one hot encoding\n",
    "    np_one_hot = np.zeros((bigram_size, bigram_size))\n",
    "    np.fill_diagonal(np_one_hot, 1)\n",
    "    bigram_one_hot = tf.constant(np_one_hot.reshape(-1), dtype=tf.float32, \n",
    "                                 shape=[bigram_size, bigram_size])\n",
    "    \n",
    "    def lstm_cell_v3 (i, o, state):\n",
    "        gate = tf.matmul(i, x) + tf.matmul(o, m) + biases\n",
    "        input_gate = tf.sigmoid(gate[:, : num_nodes])\n",
    "        forget_gate = tf.sigmoid(gate[:,num_nodes : 2 * num_nodes])\n",
    "        update = gate[:, 2 * num_nodes : 3 * num_nodes]\n",
    "        output_gate = tf.sigmoid(gate[:, 3 * num_nodes : ])\n",
    "        state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "        # add dropout here\n",
    "        output = tf.nn.dropout(output_gate * tf.tanh(state), keep_prob)\n",
    "        return output, state\n",
    "    \n",
    "    # input data\n",
    "    train_data = list()\n",
    "    for _ in range(num_unrollings + 1):\n",
    "        train_data.append(tf.placeholder(tf.int32, shape=[batch_size]))\n",
    "    train_inputs = train_data[:num_unrollings]\n",
    "    train_labels = list()\n",
    "    for i in range(1, num_unrollings + 1):\n",
    "        train_labels.append(tf.gather(bigram_one_hot, train_data[i]))\n",
    "        \n",
    "    # Unrolled LSTM loop\n",
    "    outputs = list()\n",
    "    output = saved_output\n",
    "    state = saved_state\n",
    "    for i in train_inputs:\n",
    "        output, state = lstm_cell_v3(tf.nn.embedding_lookup(embeddings, i), output, state)\n",
    "        outputs.append(output)\n",
    "    \n",
    "    # state saving across unrollings\n",
    "    with tf.control_dependencies([saved_output.assign(output),\n",
    "                                 saved_state.assign(state)]):\n",
    "        # classifier\n",
    "        logits = tf.nn.xw_plus_b(tf.concat(outputs, 0), w, b)\n",
    "        loss = tf.reduce_mean(\n",
    "            tf.nn.softmax_cross_entropy_with_logits(\n",
    "                labels=tf.concat(train_labels, 0), logits=logits))\n",
    "    # Optimizer\n",
    "    global_step = tf.Variable(0)\n",
    "    learning_rate = tf.train.exponential_decay(\n",
    "        10.0, global_step, 5000, 0.1, staircase=True)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "    gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "    optimizer = optimizer.apply_gradients(\n",
    "        zip(gradients, v), global_step=global_step)\n",
    "    \n",
    "    # predictions\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "        \n",
    "    # sampling and validation eval: batch 1, no unrolling\n",
    "    sample_input = tf.placeholder(tf.int32, shape=[1])\n",
    "    saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    reset_sample_state = tf.group(\n",
    "        saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "        saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "    embed_sample_input = tf.nn.embedding_lookup(embeddings, sample_input)\n",
    "    sample_output, sample_state = lstm_cell_v3(embed_sample_input, saved_sample_output, saved_sample_state)\n",
    "    \n",
    "    with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                 saved_sample_state.assign(sample_state)]):\n",
    "        sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch perplexity: 1024.95\n",
      "Validation set perplexity: 75847.05\n",
      "Average loss at step 100: 8.557739 learning rate: 10.000000\n",
      "Minibatch perplexity: 224.42\n",
      "Validation set perplexity: 220.12\n",
      "Average loss at step 200: 5.101782 learning rate: 10.000000\n",
      "Minibatch perplexity: 146.59\n",
      "Validation set perplexity: 151.49\n",
      "Average loss at step 300: 4.717732 learning rate: 10.000000\n",
      "Minibatch perplexity: 84.34\n",
      "Validation set perplexity: 100.27\n",
      "Average loss at step 400: 4.444198 learning rate: 10.000000\n",
      "Minibatch perplexity: 93.95\n",
      "Validation set perplexity: 91.11\n",
      "Average loss at step 500: 4.317242 learning rate: 10.000000\n",
      "Minibatch perplexity: 70.76\n",
      "Validation set perplexity: 84.58\n",
      "Average loss at step 600: 4.081284 learning rate: 10.000000\n",
      "Minibatch perplexity: 59.98\n",
      "Validation set perplexity: 46.73\n",
      "Average loss at step 700: 4.029673 learning rate: 10.000000\n",
      "Minibatch perplexity: 53.65\n",
      "Validation set perplexity: 56.53\n",
      "Average loss at step 800: 3.926848 learning rate: 10.000000\n",
      "Minibatch perplexity: 53.57\n",
      "Validation set perplexity: 56.61\n",
      "Average loss at step 900: 3.811370 learning rate: 10.000000\n",
      "Minibatch perplexity: 38.98\n",
      "Validation set perplexity: 41.19\n",
      "Average loss at step 1000: 3.765364 learning rate: 10.000000\n",
      "Minibatch perplexity: 50.20\n",
      "Validation set perplexity: 53.15\n",
      "Average loss at step 1100: 3.767848 learning rate: 10.000000\n",
      "Minibatch perplexity: 32.77\n",
      "Validation set perplexity: 49.43\n",
      "Average loss at step 1200: 3.740299 learning rate: 10.000000\n",
      "Minibatch perplexity: 40.82\n",
      "Validation set perplexity: 47.73\n",
      "Average loss at step 1300: 3.771297 learning rate: 10.000000\n",
      "Minibatch perplexity: 51.59\n",
      "Validation set perplexity: 43.02\n",
      "Average loss at step 1400: 3.732552 learning rate: 10.000000\n",
      "Minibatch perplexity: 41.60\n",
      "Validation set perplexity: 45.43\n",
      "Average loss at step 1500: 3.687863 learning rate: 10.000000\n",
      "Minibatch perplexity: 40.82\n",
      "Validation set perplexity: 40.60\n",
      "Average loss at step 1600: 3.653314 learning rate: 10.000000\n",
      "Minibatch perplexity: 33.88\n",
      "Validation set perplexity: 38.86\n",
      "Average loss at step 1700: 3.623444 learning rate: 10.000000\n",
      "Minibatch perplexity: 32.92\n",
      "Validation set perplexity: 40.23\n",
      "Average loss at step 1800: 3.621769 learning rate: 10.000000\n",
      "Minibatch perplexity: 29.69\n",
      "Validation set perplexity: 38.45\n",
      "Average loss at step 1900: 3.574014 learning rate: 10.000000\n",
      "Minibatch perplexity: 35.45\n",
      "Validation set perplexity: 40.07\n",
      "Average loss at step 2000: 3.547309 learning rate: 10.000000\n",
      "Minibatch perplexity: 31.72\n",
      "Validation set perplexity: 35.97\n",
      "Average loss at step 2100: 3.536957 learning rate: 10.000000\n",
      "Minibatch perplexity: 38.01\n",
      "Validation set perplexity: 34.08\n",
      "Average loss at step 2200: 3.493683 learning rate: 10.000000\n",
      "Minibatch perplexity: 44.94\n",
      "Validation set perplexity: 32.87\n",
      "Average loss at step 2300: 3.454955 learning rate: 10.000000\n",
      "Minibatch perplexity: 24.92\n",
      "Validation set perplexity: 35.84\n",
      "Average loss at step 2400: 3.513491 learning rate: 10.000000\n",
      "Minibatch perplexity: 30.05\n",
      "Validation set perplexity: 32.71\n",
      "Average loss at step 2500: 3.483907 learning rate: 10.000000\n",
      "Minibatch perplexity: 26.04\n",
      "Validation set perplexity: 36.19\n",
      "Average loss at step 2600: 3.486021 learning rate: 10.000000\n",
      "Minibatch perplexity: 35.63\n",
      "Validation set perplexity: 34.04\n",
      "Average loss at step 2700: 3.430146 learning rate: 10.000000\n",
      "Minibatch perplexity: 35.41\n",
      "Validation set perplexity: 30.08\n",
      "Average loss at step 2800: 3.372167 learning rate: 10.000000\n",
      "Minibatch perplexity: 40.21\n",
      "Validation set perplexity: 35.40\n",
      "Average loss at step 2900: 3.412424 learning rate: 10.000000\n",
      "Minibatch perplexity: 34.97\n",
      "Validation set perplexity: 31.18\n",
      "Average loss at step 3000: 3.367145 learning rate: 10.000000\n",
      "Minibatch perplexity: 25.24\n",
      "Validation set perplexity: 28.53\n",
      "Average loss at step 3100: 3.328576 learning rate: 10.000000\n",
      "Minibatch perplexity: 25.55\n",
      "Validation set perplexity: 34.36\n",
      "Average loss at step 3200: 3.383729 learning rate: 10.000000\n",
      "Minibatch perplexity: 27.14\n",
      "Validation set perplexity: 29.57\n",
      "Average loss at step 3300: 3.415887 learning rate: 10.000000\n",
      "Minibatch perplexity: 34.21\n",
      "Validation set perplexity: 28.79\n",
      "Average loss at step 3400: 3.391605 learning rate: 10.000000\n",
      "Minibatch perplexity: 24.34\n",
      "Validation set perplexity: 28.94\n",
      "Average loss at step 3500: 3.320395 learning rate: 10.000000\n",
      "Minibatch perplexity: 25.63\n",
      "Validation set perplexity: 26.09\n",
      "Average loss at step 3600: 3.293910 learning rate: 10.000000\n",
      "Minibatch perplexity: 23.07\n",
      "Validation set perplexity: 22.87\n",
      "Average loss at step 3700: 3.252197 learning rate: 10.000000\n",
      "Minibatch perplexity: 25.38\n",
      "Validation set perplexity: 24.30\n",
      "Average loss at step 3800: 3.227247 learning rate: 10.000000\n",
      "Minibatch perplexity: 25.02\n",
      "Validation set perplexity: 27.37\n",
      "Average loss at step 3900: 3.236686 learning rate: 10.000000\n",
      "Minibatch perplexity: 25.36\n",
      "Validation set perplexity: 26.08\n",
      "Average loss at step 4000: 3.310227 learning rate: 10.000000\n",
      "Minibatch perplexity: 33.27\n",
      "Validation set perplexity: 30.06\n",
      "Average loss at step 4100: 3.267006 learning rate: 10.000000\n",
      "Minibatch perplexity: 26.81\n",
      "Validation set perplexity: 31.76\n",
      "Average loss at step 4200: 3.284848 learning rate: 10.000000\n",
      "Minibatch perplexity: 32.97\n",
      "Validation set perplexity: 30.97\n",
      "Average loss at step 4300: 3.271511 learning rate: 10.000000\n",
      "Minibatch perplexity: 35.15\n",
      "Validation set perplexity: 23.75\n",
      "Average loss at step 4400: 3.200401 learning rate: 10.000000\n",
      "Minibatch perplexity: 24.75\n",
      "Validation set perplexity: 29.90\n",
      "Average loss at step 4500: 3.193639 learning rate: 10.000000\n",
      "Minibatch perplexity: 26.13\n",
      "Validation set perplexity: 29.84\n",
      "Average loss at step 4600: 3.239483 learning rate: 10.000000\n",
      "Minibatch perplexity: 21.13\n",
      "Validation set perplexity: 29.79\n",
      "Average loss at step 4700: 3.257970 learning rate: 10.000000\n",
      "Minibatch perplexity: 32.32\n",
      "Validation set perplexity: 20.82\n",
      "Average loss at step 4800: 3.235865 learning rate: 10.000000\n",
      "Minibatch perplexity: 27.13\n",
      "Validation set perplexity: 24.26\n",
      "Average loss at step 4900: 3.266573 learning rate: 10.000000\n",
      "Minibatch perplexity: 29.17\n",
      "Validation set perplexity: 33.91\n",
      "Average loss at step 5000: 3.289137 learning rate: 1.000000\n",
      "Minibatch perplexity: 28.22\n",
      "Validation set perplexity: 27.28\n",
      "Average loss at step 5100: 3.232039 learning rate: 1.000000\n",
      "Minibatch perplexity: 26.38\n",
      "Validation set perplexity: 29.05\n",
      "Average loss at step 5200: 3.188362 learning rate: 1.000000\n",
      "Minibatch perplexity: 30.31\n",
      "Validation set perplexity: 29.46\n",
      "Average loss at step 5300: 3.236295 learning rate: 1.000000\n",
      "Minibatch perplexity: 25.83\n",
      "Validation set perplexity: 19.46\n",
      "Average loss at step 5400: 3.257971 learning rate: 1.000000\n",
      "Minibatch perplexity: 21.04\n",
      "Validation set perplexity: 18.10\n",
      "Average loss at step 5500: 3.240418 learning rate: 1.000000\n",
      "Minibatch perplexity: 27.52\n",
      "Validation set perplexity: 22.55\n",
      "Average loss at step 5600: 3.189446 learning rate: 1.000000\n",
      "Minibatch perplexity: 27.03\n",
      "Validation set perplexity: 29.63\n",
      "Average loss at step 5700: 3.220145 learning rate: 1.000000\n",
      "Minibatch perplexity: 26.01\n",
      "Validation set perplexity: 39.35\n",
      "Average loss at step 5800: 3.212714 learning rate: 1.000000\n",
      "Minibatch perplexity: 19.09\n",
      "Validation set perplexity: 37.91\n",
      "Average loss at step 5900: 3.184047 learning rate: 1.000000\n",
      "Minibatch perplexity: 25.12\n",
      "Validation set perplexity: 37.86\n",
      "Average loss at step 6000: 3.209901 learning rate: 1.000000\n",
      "Minibatch perplexity: 22.75\n",
      "Validation set perplexity: 35.88\n",
      "Average loss at step 6100: 3.183750 learning rate: 1.000000\n",
      "Minibatch perplexity: 30.42\n",
      "Validation set perplexity: 31.40\n",
      "Average loss at step 6200: 3.169599 learning rate: 1.000000\n",
      "Minibatch perplexity: 21.38\n",
      "Validation set perplexity: 36.89\n",
      "Average loss at step 6300: 3.161056 learning rate: 1.000000\n",
      "Minibatch perplexity: 17.67\n",
      "Validation set perplexity: 27.29\n",
      "Average loss at step 6400: 3.132166 learning rate: 1.000000\n",
      "Minibatch perplexity: 21.19\n",
      "Validation set perplexity: 41.87\n",
      "Average loss at step 6500: 3.126175 learning rate: 1.000000\n",
      "Minibatch perplexity: 22.64\n",
      "Validation set perplexity: 22.34\n",
      "Average loss at step 6600: 3.141362 learning rate: 1.000000\n",
      "Minibatch perplexity: 19.19\n",
      "Validation set perplexity: 19.75\n",
      "Average loss at step 6700: 3.216189 learning rate: 1.000000\n",
      "Minibatch perplexity: 17.32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation set perplexity: 24.45\n",
      "Average loss at step 6800: 3.185761 learning rate: 1.000000\n",
      "Minibatch perplexity: 26.00\n",
      "Validation set perplexity: 25.96\n",
      "Average loss at step 6900: 3.171365 learning rate: 1.000000\n",
      "Minibatch perplexity: 21.93\n",
      "Validation set perplexity: 29.22\n",
      "Average loss at step 7000: 3.186478 learning rate: 1.000000\n",
      "Minibatch perplexity: 30.61\n",
      "Validation set perplexity: 23.73\n",
      "Average loss at step 7100: 3.197768 learning rate: 1.000000\n",
      "Minibatch perplexity: 23.17\n",
      "Validation set perplexity: 31.15\n",
      "Average loss at step 7200: 3.171031 learning rate: 1.000000\n",
      "Minibatch perplexity: 21.77\n",
      "Validation set perplexity: 25.86\n",
      "Average loss at step 7300: 3.222945 learning rate: 1.000000\n",
      "Minibatch perplexity: 28.91\n",
      "Validation set perplexity: 22.29\n",
      "Average loss at step 7400: 3.151323 learning rate: 1.000000\n",
      "Minibatch perplexity: 18.59\n",
      "Validation set perplexity: 19.74\n",
      "Average loss at step 7500: 3.121643 learning rate: 1.000000\n",
      "Minibatch perplexity: 22.56\n",
      "Validation set perplexity: 19.88\n",
      "Average loss at step 7600: 3.168654 learning rate: 1.000000\n",
      "Minibatch perplexity: 20.51\n",
      "Validation set perplexity: 21.77\n",
      "Average loss at step 7700: 3.185251 learning rate: 1.000000\n",
      "Minibatch perplexity: 22.58\n",
      "Validation set perplexity: 20.22\n",
      "Average loss at step 7800: 3.146083 learning rate: 1.000000\n",
      "Minibatch perplexity: 27.84\n",
      "Validation set perplexity: 21.55\n",
      "Average loss at step 7900: 3.194979 learning rate: 1.000000\n",
      "Minibatch perplexity: 16.20\n",
      "Validation set perplexity: 21.66\n",
      "Average loss at step 8000: 3.169461 learning rate: 1.000000\n",
      "Minibatch perplexity: 26.24\n",
      "Validation set perplexity: 22.06\n",
      "Average loss at step 8100: 3.169181 learning rate: 1.000000\n",
      "Minibatch perplexity: 19.65\n",
      "Validation set perplexity: 16.15\n",
      "Average loss at step 8200: 3.215152 learning rate: 1.000000\n",
      "Minibatch perplexity: 32.03\n",
      "Validation set perplexity: 25.69\n",
      "Average loss at step 8300: 3.188947 learning rate: 1.000000\n",
      "Minibatch perplexity: 23.51\n",
      "Validation set perplexity: 28.22\n",
      "Average loss at step 8400: 3.183024 learning rate: 1.000000\n",
      "Minibatch perplexity: 27.48\n",
      "Validation set perplexity: 27.69\n",
      "Average loss at step 8500: 3.146449 learning rate: 1.000000\n",
      "Minibatch perplexity: 19.36\n",
      "Validation set perplexity: 27.57\n",
      "Average loss at step 8600: 3.154071 learning rate: 1.000000\n",
      "Minibatch perplexity: 26.11\n",
      "Validation set perplexity: 24.71\n",
      "Average loss at step 8700: 3.156365 learning rate: 1.000000\n",
      "Minibatch perplexity: 21.44\n",
      "Validation set perplexity: 25.12\n",
      "Average loss at step 8800: 3.123572 learning rate: 1.000000\n",
      "Minibatch perplexity: 25.09\n",
      "Validation set perplexity: 28.48\n",
      "Average loss at step 8900: 3.143204 learning rate: 1.000000\n",
      "Minibatch perplexity: 17.45\n",
      "Validation set perplexity: 32.22\n",
      "Average loss at step 9000: 3.109608 learning rate: 1.000000\n",
      "Minibatch perplexity: 16.80\n",
      "Validation set perplexity: 23.57\n",
      "Average loss at step 9100: 3.104516 learning rate: 1.000000\n",
      "Minibatch perplexity: 22.00\n",
      "Validation set perplexity: 24.77\n",
      "Average loss at step 9200: 3.161872 learning rate: 1.000000\n",
      "Minibatch perplexity: 26.01\n",
      "Validation set perplexity: 25.56\n",
      "Average loss at step 9300: 3.210517 learning rate: 1.000000\n",
      "Minibatch perplexity: 23.62\n",
      "Validation set perplexity: 25.76\n",
      "Average loss at step 9400: 3.140313 learning rate: 1.000000\n",
      "Minibatch perplexity: 21.07\n",
      "Validation set perplexity: 20.77\n",
      "Average loss at step 9500: 3.132015 learning rate: 1.000000\n",
      "Minibatch perplexity: 20.43\n",
      "Validation set perplexity: 21.22\n",
      "Average loss at step 9600: 3.206312 learning rate: 1.000000\n",
      "Minibatch perplexity: 17.11\n",
      "Validation set perplexity: 23.92\n",
      "Average loss at step 9700: 3.168627 learning rate: 1.000000\n",
      "Minibatch perplexity: 23.03\n",
      "Validation set perplexity: 23.18\n",
      "Average loss at step 9800: 3.201512 learning rate: 1.000000\n",
      "Minibatch perplexity: 25.05\n",
      "Validation set perplexity: 18.94\n",
      "Average loss at step 9900: 3.070068 learning rate: 1.000000\n",
      "Minibatch perplexity: 20.52\n",
      "Validation set perplexity: 24.06\n",
      "Average loss at step 10000: 3.127642 learning rate: 0.100000\n",
      "Minibatch perplexity: 21.33\n",
      "Validation set perplexity: 29.49\n"
     ]
    }
   ],
   "source": [
    "num_steps = 10001\n",
    "summary_frequency = 100\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.initialize_all_variables().run()\n",
    "    print (\"Initialized\")\n",
    "    train_batches = BigramBatchGenerator(train_text, batch_size, num_unrollings)\n",
    "    valid_batches = BigramBatchGenerator(train_text, 1, 1)\n",
    "    mean_loss = 0\n",
    "    for step in range(num_steps):\n",
    "        batches = train_batches.next()\n",
    "        feed_dict = dict()\n",
    "        for i in range(num_unrollings + 1):\n",
    "            feed_dict[train_data[i]] = batches[i]\n",
    "        _, curr_loss, predictions, lr = session.run(\n",
    "            [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "        mean_loss += curr_loss\n",
    "        if step % summary_frequency == 0:\n",
    "            if step > 0:\n",
    "                mean_loss = mean_loss / summary_frequency\n",
    "                print('Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "            mean_loss = 0\n",
    "            labels = list(batches)[1:]\n",
    "            labels = np.concatenate([bigram2onehot(l) for l in labels])\n",
    "            print('Minibatch perplexity: %.2f' % float(np.exp(logprob(predictions, labels))))\n",
    "            # measure validation set perplexity\n",
    "            reset_sample_state.run()\n",
    "            valid_logprob = 0\n",
    "            for _ in range(valid_size):\n",
    "                b = valid_batches.next()\n",
    "                valid_predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "                valid_labels = bigram2onehot(b[1])[0].reshape(1, -1)\n",
    "                valid_logprob = valid_logprob + logprob(valid_predictions, valid_labels)\n",
    "            print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "                valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Y5tapX3kpcqZ"
   },
   "source": [
    "---\n",
    "Problem 3\n",
    "---------\n",
    "\n",
    "(difficult!)\n",
    "\n",
    "Write a sequence-to-sequence LSTM which mirrors all the words in a sentence. For example, if your input is:\n",
    "\n",
    "    the quick brown fox\n",
    "    \n",
    "the model should attempt to output:\n",
    "\n",
    "    eht kciuq nworb xof\n",
    "    \n",
    "Refer to the lecture on how to put together a sequence-to-sequence model, as well as [this article](http://arxiv.org/abs/1409.3215) for best practices.\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "6_lstm.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
